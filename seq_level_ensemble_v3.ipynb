{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604f812d-d489-4c06-8fbd-b251eca43924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for sequence-level-voting ensemble using my designed scoring mechanism with tree based similarity rather than jaccard similarity\n",
    "#This ensemble is performed on the last beams gathered from the clause-level ensemble.\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sqlglot import parse_one, exp , diff\n",
    "from sqlglot.diff import Keep\n",
    "from collections import defaultdict , Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "def jaccard_similarity(set_1 , set_2):\n",
    "    \n",
    "    intersection = len(set_1.intersection(set_2))\n",
    "    union = len(set_1.union(set_2))\n",
    "\n",
    "    if union==0:\n",
    "        return 1\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "def generator_to_set(myGenerator):\n",
    "    #It extracts the attribute .this for each element in the generator gotten from parse_one(query1).find_all(exp.Table/Column)\n",
    "    mySet = set()\n",
    "    myList = list(myGenerator)\n",
    "    for i in myList:\n",
    "        mySet.add(i.this.this)\n",
    "    return mySet\n",
    "\n",
    "def combine_similarities(column_similarity, table_similarity , tree_similarity):\n",
    "    return (column_similarity+table_similarity+tree_similarity)/3\n",
    "    \n",
    "def query_similarity(q1_parsed , q2_parsed):\n",
    "    #calculates the similarity between two queries.\n",
    "    \n",
    "    query1_columns = q1_parsed.find_all(exp.Column)\n",
    "    query1_tables = q1_parsed.find_all(exp.Table)\n",
    "    # \n",
    "    query2_columns = q2_parsed.find_all(exp.Column)\n",
    "    query2_tables = q2_parsed.find_all(exp.Table)\n",
    "    \n",
    "    #putting the extracted columns in two sets\n",
    "    q1_column_set = generator_to_set(query1_columns)\n",
    "    q2_column_set = generator_to_set(query2_columns)\n",
    "    column_similarity = jaccard_similarity( q1_column_set , q2_column_set )\n",
    "        \n",
    "    q1_table_set = generator_to_set(query1_tables)\n",
    "    q2_table_set = generator_to_set(query2_tables)\n",
    "    table_similarity = jaccard_similarity( q1_table_set , q2_table_set )\n",
    "\n",
    "    diff_list = diff(q1_parsed , q2_parsed)\n",
    "    number_of_keep = 0\n",
    "    for i in diff_list:\n",
    "        number_of_keep += int(isinstance( i , Keep ))\n",
    "    tree_similarity = number_of_keep/len(diff_list)\n",
    "    \n",
    "    return combine_similarities( column_similarity , table_similarity , tree_similarity )\n",
    "\n",
    "\n",
    "def similarity_meassure( hypothesis_query, reference_query, in_table_keywords):\n",
    "\n",
    "    hypothesis_query = hypothesis_query.replace('`' , '\"')\n",
    "    reference_query = reference_query.replace('`' , '\"')\n",
    "    try:\n",
    "        hypothesis_parsed = parse_one(hypothesis_query)\n",
    "        reference_parsed = parse_one(reference_query)\n",
    "    except:\n",
    "        return 0\n",
    "        \n",
    "    else:\n",
    "        return query_similarity( hypothesis_parsed , reference_parsed )\n",
    "\n",
    "def ensemble( decoded_text_list , inputs_log_prob , batch_text ):\n",
    "    #function for performing ensemble using the bleu metric between the candidate sequences.\n",
    "    #input:\n",
    "        #decoded_text_list: list of output strings for each candidate beam\n",
    "        #inputs_log_prob: list of torch tensor representing the probability of each input_ids with the shape( 1 , num_beam)\n",
    "        #batch_text: list of string with len()=number_components. Having the prompts for each components.\n",
    "    #return:\n",
    "        #ensembled decoded output: String\n",
    "    \n",
    "    NUMBER_of_components = len(inputs_log_prob)\n",
    "    num_beams = inputs_log_prob[0].size(-1)\n",
    "    \n",
    "    table_creation_part_prompt = batch_text[0].split('Given the following database schema:')[-1].split('Answer the following')[0]\n",
    "    # print('table_creation_part_prompt: ', table_creation_part_prompt)\n",
    "    in_table_keywords = word_tokenize( table_creation_part_prompt )\n",
    "    # print('in_table_keywords: ' , in_table_keywords)\n",
    "        \n",
    "    selection_score_list = [] #This scoring is used to select the best candidates. It uses the length penalty to calculate the scores\n",
    "    \n",
    "    #Calculating the similarity score for each candidate\n",
    "    for j in range( len( decoded_text_list ) ): #[tok_component1_beam1, tok_component2_beam1, tok_component3_beam1, ..., tok_component1_beam2, tok_component2_beam2 , ...]\n",
    "        temp_decoded_text_list = decoded_text_list.copy()\n",
    "        decoded_text = temp_decoded_text_list.pop(j)\n",
    "        selection_score = 0\n",
    "        for index , other_response in enumerate(temp_decoded_text_list):\n",
    "            if index>=j:\n",
    "                index+=1\n",
    "            # other_response_prob = torch.exp( inputs_log_prob[ int((index%self.number_of_components)+i) , 0,  int(index/self.number_of_components) ] )\n",
    "            selection_score_with_other_response = inputs_log_prob[ int(index/num_beams) ][ 0,  int(index%num_beams) ] + similarity_meassure( decoded_text, other_response, in_table_keywords )\n",
    "            \n",
    "            if selection_score == 0:\n",
    "                selection_score = selection_score_with_other_response\n",
    "            else:\n",
    "                # print(selection_score_with_other_response.dtype) torch.float32\n",
    "                # print(selection_score.dtype) torch.float32\n",
    "                alpha = max(selection_score_with_other_response , selection_score)\n",
    "                beta = min(selection_score_with_other_response , selection_score)\n",
    "                selection_score = alpha + torch.log1p(torch.exp(beta-alpha))\n",
    "                \n",
    "        selection_log_score = (selection_score + inputs_log_prob[ int(j/num_beams) ] [ 0,  int(j%num_beams) ] )#/2\n",
    "        # print(f'toknes:{tokenized_response} point:{score}')\n",
    "        selection_score_list.append(selection_log_score)\n",
    "\n",
    "    #Finding the sequence with the highest bleu score.\n",
    "    max_score_value = max( selection_score_list )\n",
    "    max_index_score = selection_score_list.index(max_score_value)\n",
    "    \n",
    "    return decoded_text_list[max_index_score]\n",
    "\n",
    "#To use the above algorithm we need the following things: decoded_text_list , inputs_log_prob , batch_text \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22b1e6b9-4861-4e27-b1cf-65f5619176c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_SAMPLES = int(len(outputs_seq)/20)\n",
    "final_output_seqs = []\n",
    "for i in range(NUMBER_OF_SAMPLES):\n",
    "    output_seq_strt_idx = i*20\n",
    "    prompts_strt_idx = i*5\n",
    "    prob_logs_strt_idx = i*5\n",
    "    decoded_text_list = outputs_seq[ output_seq_strt_idx : output_seq_strt_idx + 1 ]\n",
    "    decoded_text_list.extend(outputs_seq[ output_seq_strt_idx+4 : output_seq_strt_idx + 5 ])\n",
    "    decoded_text_list.extend(outputs_seq[ output_seq_strt_idx+8 : output_seq_strt_idx + 9 ])\n",
    "    decoded_text_list.extend(outputs_seq[ output_seq_strt_idx+12 : output_seq_strt_idx + 13 ])\n",
    "    decoded_text_list.extend(outputs_seq[ output_seq_strt_idx+16 : output_seq_strt_idx + 17 ])\n",
    "    inputs_log_prob = [ i[ : , 0:1 ] for i in outputs_log_prob[ prob_logs_strt_idx : prob_logs_strt_idx + 5 ] ]\n",
    "    batch_text = prompts[ prompts_strt_idx : prompts_strt_idx + 5 ]\n",
    "    final_output = ensemble( decoded_text_list , inputs_log_prob , batch_text )\n",
    "    final_output_seqs.append( final_output )\n",
    "with open('./final_output_weighted_treebased_scoring.pkl' , 'wb')as f:\n",
    "    pkl.dump(final_output_seqs , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79b8d4b7-b2b3-4091-b449-8f7500d86c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yadegari/miniconda3/envs/yadegari_cpu/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "#grouping the pieces of output_sequences\n",
    "import pickle as pkl\n",
    "\n",
    "def gather_threads_outputs(dir_name , post_fix):\n",
    "    #dir_name = 'outputs1'\n",
    "    #post_fix = '_batch_text' or '_inputs_log_prob' or ''\n",
    "    #grouping the pieces of output_sequences\n",
    "    prefix = './'+ dir_name + '/output_sequences_'\n",
    "    output_sequences = []\n",
    "    for i in range(0 , 1050 , 50):\n",
    "        if i <1000:\n",
    "            if i == 0: \n",
    "                file_name = prefix + '0_'  + str(i+50).lstrip('0') + '-4'+ post_fix +'.pkl'\n",
    "            else:\n",
    "                file_name = prefix + str(i).lstrip('0') + '_' + str(i+50).lstrip('0') + '-4'+ post_fix +'.pkl'\n",
    "        else:\n",
    "            file_name = prefix + str(i).lstrip('0') + '_end' + '-4'+ post_fix +'.pkl'\n",
    "        # print('Processing file: ' , file_name )\n",
    "        with open(file_name , 'rb') as f:\n",
    "            part_of_output = pkl.load(f)\n",
    "            # print('Length of the processed output file: ' , len(part_of_output))\n",
    "        output_sequences.extend(part_of_output)\n",
    "\n",
    "    # print( 'Output length is: ' , len(output_sequences) )\n",
    "    return output_sequences\n",
    "\n",
    "output_dir = 'outputs1'\n",
    "outputs_seq = gather_threads_outputs(output_dir , '') # 20 answers for each question. 0-19 question 1, etc.\n",
    "prompts = gather_threads_outputs(output_dir , '_batch_text') # 5 prompts for each question all in one list: 0-4 for question 1, 5-9 for question 2, and etc.\n",
    "outputs_log_prob = gather_threads_outputs(output_dir , '_inputs_log_prob') # 5 prompts for each question all in one list: 0-4 for question 1, 5-9 for question 2, and etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
