{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e271cbbe-e42d-4e30-8cbc-75a0442fb470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# with open('./DAIL-SQL/dataset/process/SPIDER-TEST_SQL_0-SHOT_CTX-200_ANS-2048/questions.json' , 'r') as f:\n",
    "#     generated_prompts_file_byte = f.read()\n",
    "#     generated_prompts = json.loads(generated_prompts_file_byte)\n",
    "\n",
    "with open('./codeS_pred/ENSEMBLE_seqLevelVote_SQLscore_BIRD-TEST_SQL_1_3_5-SHOT_EUCDISQUESTIONMASK_QA-EXAMPLE_CTX-200_ANS-2048_codeS_1b.json' , 'r') as f:\n",
    "    generated_response_file_byte = f.read()\n",
    "    generated_response = json.loads(generated_response_file_byte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc89654e-3f05-4532-a0d2-cf86cd8ac075",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./llama_pred/BIRD-TEST_SQL_0-SHOT_CTX-200_ANS-2048_evidence_Llama_7b.txt' , 'w')as f:\n",
    "    for i in generated_response['questions']:\n",
    "        f.write( i['response']+'\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ece1eda-d0e4-47b9-bd09-dd00834ed478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prompt_tokens', 'prompt', 'response', 'n_examples', 'db_id'])\n"
     ]
    }
   ],
   "source": [
    "#Code to make the dataset (DAIL style dataset) from CodeS predictions and the generated prompts\n",
    "with open('./llama_pred/BIRD-TEST_SQL_5-SHOT_EUCDISMASKPRESKLSIMTHR_QA-EXAMPLE_CTX-200_ANS-2048_Llama_7b.json' , 'r') as f:\n",
    "    file_bytes = f.read()\n",
    "    reference_json = json.loads(file_bytes)\n",
    "\n",
    "print(reference_json['questions'][0].keys())\n",
    "\n",
    "\n",
    "with open('./codeS_pred/pred_codes-1b_BIRD_table_num_5_column_num_6_5-shot_max_tokens_8192_max_new_tokens_256.json' , 'r') as f:\n",
    "    file_bytes = f.read()\n",
    "    pred_json = json.loads(file_bytes)\n",
    "i = 0\n",
    "\n",
    "with open('./codeS_pred/prompts_codes-1b_BIRD_table_num_5_column_num_6_5-shot_max_tokens_8192_max_new_tokens_256.json' , 'r') as f:\n",
    "    file_bytes = f.read()\n",
    "    prompts_json = json.loads(file_bytes)\n",
    "\n",
    "for i in pred_json.keys():\n",
    "    response = pred_json[i]\n",
    "    prompts = prompts_json[i]\n",
    "    reference_json['questions'][int(i)]['prompt'] = prompts\n",
    "    reference_json['questions'][int(i)]['response'] = response.split('----- bird -----')[0]\n",
    "\n",
    "with open('./codeS_pred/codes-1b_BIRD_table_num_5_column_num_6_5-shot_max_tokens_8192_max_new_tokens_256.json' , 'w' )as f:\n",
    "    json.dump(reference_json , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afe73306-a49b-453b-b3ad-496891b9ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing the demonstrations into multiple dataset to make them our components for ensemble\n",
    "#reading a reference bird file. We won't care about the response part but we only care about the prompt section and the answer id\n",
    "import copy\n",
    "\n",
    "with open('./codeS_pred/codes-1b_BIRD_table_num_5_column_num_6_5-shot_max_tokens_8192_max_new_tokens_256.json' , 'r') as f:\n",
    "    file_bytes = f.read()\n",
    "    reference_json = json.loads(file_bytes)\n",
    "\n",
    "ex_per_component = 5\n",
    "components_name = [ 'codes-1b_BIRD_table_num_5_column_num_6_5-shot_0-5_max_tokens_8192_max_new_tokens_256.json',\n",
    "                    'codes-1b_BIRD_table_num_5_column_num_6_5-shot_5-10_max_tokens_8192_max_new_tokens_256.json',\n",
    "                    'codes-1b_BIRD_table_num_5_column_num_6_5-shot_10-15_max_tokens_8192_max_new_tokens_256.json',\n",
    "                    'codes-1b_BIRD_table_num_5_column_num_6_5-shot_15-20_max_tokens_8192_max_new_tokens_256.json',\n",
    "                    'codes-1b_BIRD_table_num_5_column_num_6_5-shot_20-25_max_tokens_8192_max_new_tokens_256.json'\n",
    "                  ]\n",
    "output_datasets = [ copy.deepcopy(reference_json) for i in range( len(components_name) ) ]\n",
    "\n",
    "with open('./CodeS/codes/prompts_codes-1b_BIRD_table_num_5_column_num_6_25-shot_max_tokens_40960_max_new_tokens_256.json' , 'r') as f:\n",
    "    file_bytes = f.read()\n",
    "    pred_json = json.loads(file_bytes)\n",
    "\n",
    "\n",
    "for i in range( len(pred_json.keys()) ):\n",
    "    prompt = pred_json[str(i)]\n",
    "    splitting_term = 'database schema :'\n",
    "    prompt_list = prompt.split( splitting_term )\n",
    "    the_question = splitting_term + prompt_list[-1]\n",
    "    examples = prompt_list[1:-1]\n",
    "    last_notprocessd_ex = 0\n",
    "    for component_index in range( len(components_name) ):\n",
    "        prompt = ''\n",
    "        for ex_index in range(ex_per_component):\n",
    "            prompt = prompt + splitting_term + examples[ last_notprocessd_ex + ex_index ]\n",
    "        prompt = prompt + the_question\n",
    "        last_notprocessd_ex += ex_per_component\n",
    "        output_datasets[component_index]['questions'][i]['prompt'] = prompt\n",
    "\n",
    "for index , component_name in enumerate(components_name):\n",
    "    with open( './components/' + component_name , 'w' )as f:\n",
    "        json.dump(output_datasets[index] , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "899ffe40-d041-4327-92b3-efd14c9409b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('output_sequences-1.pkl', 'rb') as f:  # open a text file\n",
    "    output_sequences = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f60b7b7-62e4-4293-8cae-ece743cdd1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "def extract_prompt_from_list_of_questions(question_list):\n",
    "    batch_list = []\n",
    "    for i in question_list:\n",
    "        batch_list.append(i['prompt'])\n",
    "    return batch_list\n",
    "\n",
    "from utils.post_process import get_exec_output\n",
    "\n",
    "import psutil\n",
    "def query_to_db(query , db_id , db_dir ):\n",
    "    db_path = f\"{db_dir}/{db_id}/{db_id}\"\n",
    "    flag, denotation = get_exec_output(\n",
    "            db_path,\n",
    "            query)\n",
    "    \n",
    "    return flag, denotation\n",
    "\n",
    "from threading import Thread\n",
    "import threading\n",
    "import ctypes\n",
    "class Thread_with_exception(Thread):\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs={}, Verbose=None , daemon=False):\n",
    "        Thread.__init__(self, group, target, name, args, kwargs , daemon=daemon)\n",
    "        \n",
    "        self._return = None\n",
    "            \n",
    "    def run(self):\n",
    "        if self._target is not None:\n",
    "            self._return = self._target(*self._args,\n",
    "                                                **self._kwargs)\n",
    "         \n",
    "    def get_id(self):\n",
    "        # returns id of the respective thread\n",
    "        if hasattr(self, '_thread_id'):\n",
    "            return self._thread_id\n",
    "        for id, thread in threading._active.items():\n",
    "            if thread is self:\n",
    "                return id\n",
    "\n",
    "    def join(self, *args):\n",
    "        Thread.join(self, *args)\n",
    "        return self._return\n",
    " \n",
    "    def raise_exception(self):\n",
    "        thread_id = self.get_id()\n",
    "        res = ctypes.pythonapi.PyThreadState_SetAsyncExc(thread_id, ctypes.py_object(SystemExit))\n",
    "        if res > 1:\n",
    "            ctypes.pythonapi.PyThreadState_SetAsyncExc(thread_id, 0)\n",
    "            print('Exception raise failure')\n",
    "            \n",
    "\n",
    "import time\n",
    "from utils.post_process import result_eq\n",
    "\n",
    "def is_queries_equal(testing_query , ground_truth_query , db_id , db_dir, timeout_time , gt_results = None):\n",
    "    #Input:\n",
    "        #time_out_time: integer: time in seconds\n",
    "    \n",
    "    if ground_truth_query!='':\n",
    "        with open('./log.txt', 'a') as f:\n",
    "            f.write(f\"procceing the ground_truth_query:\\n{ground_truth_query}\\n\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        gt_flag, gt_denotation = query_to_db(ground_truth_query , db_id , db_dir)\n",
    "    \n",
    "        gt_process_time = time.time() - start_time\n",
    "        with open('./log.txt', 'a') as f:\n",
    "            f.write(f\"processing time: {gt_process_time}\\n\\n\")\n",
    "        gt_results = (gt_flag, gt_denotation , gt_process_time)\n",
    "        \n",
    "    else:\n",
    "        gt_flag, gt_denotation, gt_process_time = gt_results\n",
    "\n",
    "    max_timeout_time = max( timeout_time , 2*gt_process_time )\n",
    "    print('query max timeout: ' , max_timeout_time)\n",
    "\n",
    "    test_thread = Thread_with_exception(target= query_to_db , args = (testing_query , db_id , db_dir ) , daemon=True )\n",
    "    with open('./log.txt', 'a') as f:\n",
    "        f.write(f\"proccessing the testing_query:\\n{testing_query}\\n\")\n",
    "    start_time = time.time()\n",
    "    test_thread.start()\n",
    "    test_result = test_thread.join(max_timeout_time)\n",
    "    if test_thread.is_alive():\n",
    "        with open('./log.txt', 'a') as f:\n",
    "            f.write(f\"**********processing is terminated due to timeout. max_timeout_time: {max_timeout_time}\\n\")\n",
    "        test_thread.raise_exception()\n",
    "        test_thread.join()\n",
    "        return False, gt_results\n",
    "        \n",
    "    test_flag, test_denotation = test_result\n",
    "    with open('./log.txt', 'a') as f:\n",
    "        f.write(f\"Processing time for the testing query: {time.time() - start_time}\\n\\n\")\n",
    "    \n",
    "    if gt_flag[0] != 'result':\n",
    "        with open('./log.txt', 'a') as f:\n",
    "            f.write(f\"!!!!!!!!!!The following ground truth has an error:\\n{ground_truth_query}\\n\\n\")\n",
    "        return False, gt_results\n",
    "    elif test_flag[0] != 'result':\n",
    "        return False, gt_results\n",
    "    elif 'ORDER BY' in ground_truth_query or 'order by' in ground_truth_query:\n",
    "        is_equal = result_eq(gt_flag[1] , test_flag[1] , order_matters=True)\n",
    "    else:\n",
    "        is_equal = result_eq(gt_flag[1] , test_flag[1] , order_matters=False)\n",
    "    \n",
    "    return is_equal, gt_results\n",
    "    \n",
    "import os\n",
    "def put_responses_back_to_json_dataset(index , json_dataset , sequences, dataset_type='spider' , timeout_time=1):#dataset_type=spider/bird\n",
    "    #Inputs:\n",
    "        #sequences: List of [ { 'generated_text' : gen_text } ] or [gen_text1 , gen_text2 , ...]\n",
    "        #json_dataset: json dataset that contains the ground truth in its 'response' part\n",
    "            #If the elements in input sequences are like seq[0]['generated_text'] then json_dataset should contain the prompt in 'prompts' part\n",
    "    gt_result_cache_file = './cache/' + dataset_type + '_results.pkl'\n",
    "    gt_results_is_cached = False\n",
    "    gt_results_list = []\n",
    "    \n",
    "    if os.path.isfile(gt_result_cache_file):\n",
    "        gt_results_is_cached = True\n",
    "        with open(gt_result_cache_file , 'rb' )as f:\n",
    "            gt_results_list = pkl.load(f)\n",
    "\n",
    "    db_dir = './DAIL-SQL/dataset/'+ dataset_type +'/database'\n",
    "    execution_accuracy = 0\n",
    "    \n",
    "    for i in range( 0, len(sequences), 1 ):\n",
    "        seq = sequences[i]\n",
    "        # print(f\"Number of processed sequences: {i}\\t|\\tNumber of correct queries: {execution_accuracy} \\n\")\n",
    "        # if i%10==0:\n",
    "        with open('./log.txt', 'a') as f:\n",
    "            f.write(f\"Number of processed sequences: {i}\\t|\\tNumber of correct queries: {execution_accuracy} \\n\")\n",
    "            \n",
    "        prompt_len = len ( json_dataset['questions'][index+i]['prompt'] )\n",
    "        if isinstance(seq, list):\n",
    "            gen_text = seq[0]['generated_text'][prompt_len:]\n",
    "        else:\n",
    "            # gen_text = seq[prompt_len:]\n",
    "            gen_text = seq\n",
    "            \n",
    "        processed_gen_text = post_process_get_sql_from_gentext(gen_text)\n",
    "        \n",
    "        db_id = json_dataset['questions'][index+i]['db_id']\n",
    "\n",
    "        if dataset_type=='spider':\n",
    "\n",
    "            if gt_results_is_cached:\n",
    "                is_equal , gt_results = is_queries_equal(processed_gen_text , '' , db_id , db_dir, timeout_time , gt_results = gt_results_list[i])\n",
    "            else:\n",
    "                ground_truth = post_process_get_sql_from_gentext( json_dataset['questions'][index+i]['response'] )\n",
    "                is_equal , gt_results = is_queries_equal(processed_gen_text , ground_truth , db_id , db_dir, timeout_time )\n",
    "                gt_results_list.append(gt_results)\n",
    "            execution_accuracy += is_equal\n",
    "            \n",
    "        json_dataset['questions'][index+i]['response'] = processed_gen_text\n",
    "        # print('is_equal: ', is_equal)\n",
    "        # print('--------------------------')\n",
    "    if not gt_results_is_cached:\n",
    "        with open(gt_result_cache_file , 'wb' )as f:\n",
    "            pkl.dump( gt_results_list , f )\n",
    "        \n",
    "    return execution_accuracy\n",
    "\n",
    "from utils.post_process import process_duplication\n",
    "def post_process_get_sql_from_gentext(gen_text):\n",
    "    # remove \\n and extra spaces\n",
    "    # print(gen_text)\n",
    "    sql = \" \".join(gen_text.replace(\"\\n\", \" \").split())\n",
    "    sql = process_duplication(sql)\n",
    "    # python version should >= 3.8\n",
    "    if sql.startswith(\"SELECT\"):\n",
    "        sql = sql\n",
    "    elif sql.startswith(\" \"):\n",
    "        sql = \"SELECT\" + sql\n",
    "    else:\n",
    "        sql = \"SELECT \" + sql\n",
    "    return sql\n",
    "\n",
    "data_size=1034\n",
    "def eval_list_sql(sql_list , groundtruth_json_file_name , output_filename='' , dataset_type='spider' ):\n",
    "    #This function gets a list of sql predictions and evalueates it and creates a dail style dataset if the output_file_name is given\n",
    "    # groundtruth_json_file_name: Address of a dail style dataset containing the groundtruth in its response part.\n",
    "        #If the sql_list elements are of shape seq[0]['generated_text'] groundtruth_json_file_name should have the prompts too\n",
    "    with open(groundtruth_json_file_name , 'r') as f:\n",
    "        generated_prompts_file_byte = f.read()\n",
    "        generated_prompts = json.loads(generated_prompts_file_byte)\n",
    "        exec_acc = put_responses_back_to_json_dataset( 0 , generated_prompts , sql_list , dataset_type=dataset_type )\n",
    "    print('execution accuracy = ' , exec_acc/data_size)\n",
    "    with open('./log.txt', 'a') as f:\n",
    "        f.write(f\"execution accuracy = {exec_acc/data_size} \\n\")\n",
    "    if output_filename !='':\n",
    "        with open( output_filename , 'w' )as f:\n",
    "            json.dump(generated_prompts , f)\n",
    "\n",
    "def dail_dataset_to_response_list( dail_style_dataset_name ):\n",
    "    #This function gets a dail-style dataset and returns the all the responses in that dataset in a list.\n",
    "    return_list = []\n",
    "    with open( dail_style_dataset_name , 'r') as f:\n",
    "        generated_response_file_byte = f.read()\n",
    "        generated_response = json.loads(generated_response_file_byte)\n",
    "    for i in generated_response['questions']:\n",
    "        return_list.append( i['response'] )\n",
    "    return return_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0946f24a-1343-4624-87f9-5036fc122d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution accuracy =  0.42649903288201163\n"
     ]
    }
   ],
   "source": [
    "#re-evaluating a dataset that is created before\n",
    "import json\n",
    "groundtruth_json_file_name = './DAIL-SQL/dataset/process/SPIDER-TEST_SQL_0-SHOT_CTX-200_ANS-2048/questions.json'\n",
    "\n",
    "evaluating_dataset = './llama_pred/SPIDER_beam_4_lenpen0-MBRdbScore-TEST_SQL_3-SHOT-5Components_ranks0_4_8_12_16_EUCDISMASKPRESKLSIMTHR_QA-EXAMPLE_CTX-200_ANS-2048_llama_7b_v2.json'\n",
    "\n",
    "output_sequences = dail_dataset_to_response_list( evaluating_dataset )\n",
    "\n",
    "eval_list_sql(output_sequences , groundtruth_json_file_name , output_filename='' , dataset_type='spider' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fc11f3d-c76f-4707-bfb8-e72beb0eb57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n",
      "query max timeout:  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m groundtruth_json_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./DAIL-SQL/dataset/process/SPIDER-TEST_SQL_3-SHOT_EUCDISMASKPRESKLSIMTHR_QA-EXAMPLE_CTX-200_ANS-2048_llama_7b/questions.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# with open('./output_sequences-5_lenpen1.pkl', 'rb') as f:  # open a text file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     output_sequences = pkl.load(f)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43meval_list_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_sequences\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroundtruth_json_file_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m              \u001b[49m\u001b[43moutput_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./llama_pred/SPIDER_beam_4_lenpen01-cluase_step_MBMBR_weighted-tree-based_beams0_4_8_12_16-TEST_SQL_3-SHOT-5Components_EUCDISMASKPRESKLSIMTHR_QA-EXAMPLE_CTX-200_ANS-2048_llama_7bv2.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspider\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 188\u001b[0m, in \u001b[0;36meval_list_sql\u001b[0;34m(sql_list, groundtruth_json_file_name, output_filename, dataset_type)\u001b[0m\n\u001b[1;32m    186\u001b[0m     generated_prompts_file_byte \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    187\u001b[0m     generated_prompts \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(generated_prompts_file_byte)\n\u001b[0;32m--> 188\u001b[0m     exec_acc \u001b[38;5;241m=\u001b[39m \u001b[43mput_responses_back_to_json_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_prompts\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_list\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_type\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecution accuracy = \u001b[39m\u001b[38;5;124m'\u001b[39m , exec_acc\u001b[38;5;241m/\u001b[39mdata_size)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./log.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[2], line 149\u001b[0m, in \u001b[0;36mput_responses_back_to_json_dataset\u001b[0;34m(index, json_dataset, sequences, dataset_type, timeout_time)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_type\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspider\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gt_results_is_cached:\n\u001b[0;32m--> 149\u001b[0m         is_equal , gt_results \u001b[38;5;241m=\u001b[39m \u001b[43mis_queries_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_gen_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgt_results_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m         ground_truth \u001b[38;5;241m=\u001b[39m post_process_get_sql_from_gentext( json_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestions\u001b[39m\u001b[38;5;124m'\u001b[39m][index\u001b[38;5;241m+\u001b[39mi][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m] )\n",
      "Cell \u001b[0;32mIn[2], line 90\u001b[0m, in \u001b[0;36mis_queries_equal\u001b[0;34m(testing_query, ground_truth_query, db_id, db_dir, timeout_time, gt_results)\u001b[0m\n\u001b[1;32m     88\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**********processing is terminated due to timeout. max_timeout_time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_timeout_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m     test_thread\u001b[38;5;241m.\u001b[39mraise_exception()\n\u001b[0;32m---> 90\u001b[0m     \u001b[43mtest_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, gt_results\n\u001b[1;32m     93\u001b[0m test_flag, test_denotation \u001b[38;5;241m=\u001b[39m test_result\n",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m, in \u001b[0;36mThread_with_exception.join\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mThread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return\n",
      "File \u001b[0;32m~/miniconda3/envs/yadegari_cpu/lib/python3.12/threading.py:1147\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/yadegari_cpu/lib/python3.12/threading.py:1167\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1168\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#evaluating some generated prompts recently generated. This is for SPIDER\n",
    "import pickle as pkl\n",
    "\n",
    "groundtruth_json_file_name = './DAIL-SQL/dataset/process/SPIDER-TEST_SQL_3-SHOT_EUCDISMASKPRESKLSIMTHR_QA-EXAMPLE_CTX-200_ANS-2048_llama_7b/questions.json'\n",
    "\n",
    "# with open('./output_sequences-5_lenpen1.pkl', 'rb') as f:  # open a text file\n",
    "#     output_sequences = pkl.load(f)\n",
    "\n",
    "eval_list_sql(output_sequences , groundtruth_json_file_name , \n",
    "              output_filename='./llama_pred/SPIDER_beam_4_lenpen01-cluase_step_MBMBR_weighted-tree-based_beams0_4_8_12_16-TEST_SQL_3-SHOT-5Components_EUCDISMASKPRESKLSIMTHR_QA-EXAMPLE_CTX-200_ANS-2048_llama_7bv2.json' , dataset_type='spider' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bfa9d98-9df6-4e10-b136-85b556ba5925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution accuracy =  0.0\n"
     ]
    }
   ],
   "source": [
    "#evaluating some generated prompts recently generated. This is for BIRD\n",
    "import pickle as pkl\n",
    "\n",
    "groundtruth_json_file_name = './DAIL-SQL/dataset/process/BIRD-TEST_SQL_0-SHOT_CTX-200_ANS-2048/questions.json'\n",
    "\n",
    "# with open('./output_sequences-5_lenpen1.pkl', 'rb') as f:  # open a text file\n",
    "#     output_sequences = pkl.load(f)\n",
    "\n",
    "eval_list_sql(output_sequences , groundtruth_json_file_name , \n",
    "              output_filename='./codeS_pred/codes-1b_beam4_lenpen05_BIRD_table_num_5_column_num_6_5-shot_20_25_max_tokens_8192_max_new_tokens_256.json' , dataset_type='bird' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f736283f-f1ab-4f2d-9873-c740c6b1a656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./outputs1/output_sequences_0_50-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_50_100-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_100_150-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_150_200-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_200_250-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_250_300-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_300_350-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_350_400-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_400_450-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_450_500-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_500_550-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_550_600-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_600_650-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_650_700-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_700_750-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_750_800-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_800_850-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_850_900-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_900_950-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_950_1000-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_1000_end-4.pkl\n",
      "680\n",
      "20680\n"
     ]
    }
   ],
   "source": [
    "#grouping the pieces of output_sequences\n",
    "import pickle as pkl\n",
    "prefix = './outputs1/output_sequences_'\n",
    "output_sequences = []\n",
    "for i in range(0 , 1050 , 50):\n",
    "    if i <1000:\n",
    "        if i == 0: \n",
    "            file_name = prefix + '0_'  + str(i+50).lstrip('0') + '-4.pkl'\n",
    "        else:\n",
    "            file_name = prefix + str(i).lstrip('0') + '_' + str(i+50).lstrip('0') + '-4.pkl'\n",
    "    else:\n",
    "        file_name = prefix + str(i).lstrip('0') + '_end' + '-4.pkl'\n",
    "    print(file_name)\n",
    "    with open(file_name , 'rb') as f:\n",
    "        part_of_output = pkl.load(f)\n",
    "        print(len(part_of_output))\n",
    "        # new_part_of_output = []\n",
    "        # if len(part_of_output)==170:\n",
    "        #     for x in range(0,len(part_of_output),5):\n",
    "        #         new_part_of_output.append(x)\n",
    "        #     with open('./new_'+file_name[2:], 'wb')as h:\n",
    "        #         pkl.dump(new_part_of_output , h)\n",
    "    output_sequences.extend(part_of_output)\n",
    "\n",
    "print( len(output_sequences) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54c529b8-170b-4de2-96ae-9b99e29c8211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1534\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "file_name = './final_output_CodeS_preprocess_ensemble.pkl'\n",
    "with open(file_name , 'rb') as f:\n",
    "    output_sequences = pkl.load(f)\n",
    "\n",
    "print(len(output_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c4fc8fd4-74ca-43c7-98f9-bb7a39f4fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequences_list = [[] for i in range(20)]\n",
    "for i in range(20):\n",
    "    for j in range(i, len(output_sequences) , 20):\n",
    "        output_sequences_list[i].append( output_sequences[j] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
