{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e271cbbe-e42d-4e30-8cbc-75a0442fb470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The code to transfer DAIL-SQL question organization to OneSQL organization.\n",
    "import json\n",
    "with open('./DAIL-SQL/dataset/process/BIRD-TEST_SQL_0-SHOT_CTX-200_ANS-2048_wEvidence/questions.json' , 'r') as f:\n",
    "    generated_prompts_file_byte = f.read()\n",
    "    generated_prompts = json.loads(generated_prompts_file_byte)\n",
    "\n",
    "for idx, question in enumerate(generated_prompts['questions']):\n",
    "    prompt = question['prompt']\n",
    "    prompt = prompt.replace('/* Given the following database schema: */', '' )\n",
    "    prompt = prompt.replace('/* Answer the following:' , '--')\n",
    "    prompt = prompt.replace(' */' , '')\n",
    "    if idx<5:\n",
    "        print('Before: ' , question['prompt'])\n",
    "        print('After : ' , prompt)\n",
    "    generated_prompts['questions'][idx]['prompt'] = prompt\n",
    "\n",
    "\n",
    "with open('./components/BIRD-TEST_SQL_0-SHOT_OneSQLStyle_wEvidence.json' , 'w' )as f:\n",
    "    json.dump(generated_prompts , f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc89654e-3f05-4532-a0d2-cf86cd8ac075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting OmniSQL style devset to codeS style devset.\n",
    "with open('./Qwen_pred/SFTCodeS-7b_beam4_BIRD_wEvidence_0-5shot_CodeSORG.json' , 'r') as f:\n",
    "    file_bytes = f.read()\n",
    "    reference_json = json.loads(file_bytes)\n",
    "\n",
    "with open('./OmniSQL/train_and_evaluate/data/dev_bird.json' , 'r') as f:\n",
    "    file_bytes = f.read()\n",
    "    prompts_json = json.loads(file_bytes)\n",
    "\n",
    "for i in range(len(prompts_json)):\n",
    "    prompts = prompts_json[i]['input_seq']\n",
    "    reference_json['questions'][int(i)]['prompt'] = prompts\n",
    "\n",
    "with open('./components/OmniSQL_BIRD_0-shot_wEvidence.json' , 'w' )as f:\n",
    "    json.dump(reference_json , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6ece1eda-d0e4-47b9-bd09-dd00834ed478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['prompt_tokens', 'prompt', 'response', 'n_examples', 'db_id'])\n"
     ]
    }
   ],
   "source": [
    "#Code to make the dataset (DAIL style dataset) from CodeS predictions and the generated prompts\n",
    "with open('./Qwen_pred/SFTCodeS-7b_beam4_BIRD_wEvidence_0-5shot_CodeSORG.json' , 'r') as f:\n",
    "    file_bytes = f.read()\n",
    "    reference_json = json.loads(file_bytes)\n",
    "\n",
    "print(reference_json['questions'][0].keys())\n",
    "\n",
    "\n",
    "with open('./codeS_pred/pred_codes-1b_BIRD_table_num_5_column_num_6_5-shot_max_tokens_8192_max_new_tokens_256.json' , 'r') as f:\n",
    "    file_bytes = f.read()\n",
    "    pred_json = json.loads(file_bytes)\n",
    "i = 0\n",
    "\n",
    "with open('./CodeS/codes/prompts_codes-1b_EVIDENCE_table_num_5_column_num_6_1-shot_max_tokens_8192_max_new_tokens_256.json' , 'r') as f:\n",
    "    file_bytes = f.read()\n",
    "    prompts_json = json.loads(file_bytes)\n",
    "\n",
    "for i in pred_json.keys():\n",
    "    response = pred_json[i]\n",
    "    prompts = prompts_json[i]\n",
    "    reference_json['questions'][int(i)]['prompt'] = prompts\n",
    "    reference_json['questions'][int(i)]['response'] = response.split('----- bird -----')[0]\n",
    "\n",
    "with open('./components/codes-1b_BIRD_table_num_5_column_num_6_1-shot_wEvidence_max_tokens_8192_max_new_tokens_256.json' , 'w' )as f:\n",
    "    json.dump(reference_json , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "afe73306-a49b-453b-b3ad-496891b9ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing the demonstrations into multiple dataset to make them our components for ensemble\n",
    "#reading a reference bird file. We won't care about the response part but we only care about the prompt section and the answer id\n",
    "import random\n",
    "import copy\n",
    "\n",
    "with open('./codeS_pred/codes-1b_BIRD_table_num_5_column_num_6_5-shot_max_tokens_8192_max_new_tokens_256.json' , 'r') as f:\n",
    "    file_bytes = f.read()\n",
    "    reference_json = json.loads(file_bytes)\n",
    "\n",
    "ex_per_component = 5\n",
    "components_name = [ 'codes-1b_BIRD_table_num_6_column_num_10_5-shot_0-5_wEvidence_max_tokens_8192_max_new_tokens_256.json',\n",
    "                    'codes-1b_BIRD_table_num_6_column_num_10_5-shot_5-10_wEvidence_max_tokens_8192_max_new_tokens_256.json',\n",
    "                    'codes-1b_BIRD_table_num_6_column_num_10_5-shot_10-15_wEvidence_max_tokens_8192_max_new_tokens_256.json',\n",
    "                    'codes-1b_BIRD_table_num_6_column_num_10_5-shot_15-20_wEvidence_max_tokens_8192_max_new_tokens_256.json',\n",
    "                    'codes-1b_BIRD_table_num_6_column_num_10_5-shot_20-25_wEvidence_max_tokens_8192_max_new_tokens_256.json'\n",
    "                  ]\n",
    "output_datasets = [ copy.deepcopy(reference_json) for i in range( len(components_name) ) ]\n",
    "\n",
    "with open('./CodeS/codes/prompts_codes-1b_EVIDENCE_table_num_6_column_num_10_25-shot_max_tokens_8192_max_new_tokens_256.json' , 'r') as f:\n",
    "    file_bytes = f.read()\n",
    "    pred_json = json.loads(file_bytes)\n",
    "\n",
    "\n",
    "for i in range( len(pred_json.keys()) ):\n",
    "    prompt = pred_json[str(i)]\n",
    "    splitting_term = 'database schema :'\n",
    "    prompt_list = prompt.split( splitting_term )\n",
    "    the_question = splitting_term + prompt_list[-1]\n",
    "    examples = prompt_list[1:-1]\n",
    "    last_notprocessd_ex = 0\n",
    "    for component_index in range( len(components_name) ):\n",
    "        prompt = ''\n",
    "        for ex_index in range(ex_per_component):\n",
    "            # chosen_example = random.choice(examples) # if you wanna select the examples randomly\n",
    "            # examples.remove(chosen_example) # if you wanna select the examples randomly\n",
    "            # prompt = prompt + splitting_term + chosen_example\n",
    "            prompt = prompt + splitting_term + examples[ last_notprocessd_ex + ex_index ] #if the examples are supposed to be in order for each component\n",
    "        prompt = prompt + the_question\n",
    "        last_notprocessd_ex += ex_per_component\n",
    "        output_datasets[component_index]['questions'][i]['prompt'] = prompt\n",
    "\n",
    "for index , component_name in enumerate(components_name):\n",
    "    with open( './components/' + component_name , 'w' )as f:\n",
    "        json.dump(output_datasets[index] , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "899ffe40-d041-4327-92b3-efd14c9409b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('output_sequences-1.pkl', 'rb') as f:  # open a text file\n",
    "    output_sequences = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f60b7b7-62e4-4293-8cae-ece743cdd1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "def extract_prompt_from_list_of_questions(question_list):\n",
    "    batch_list = []\n",
    "    for i in question_list:\n",
    "        batch_list.append(i['prompt'])\n",
    "    return batch_list\n",
    "\n",
    "from utils.post_process import get_exec_output\n",
    "\n",
    "import psutil\n",
    "def query_to_db(query , db_id , db_dir ):\n",
    "    db_path = f\"{db_dir}/{db_id}/{db_id}\"\n",
    "    flag, denotation = get_exec_output(\n",
    "            db_path,\n",
    "            query)\n",
    "    \n",
    "    return flag, denotation\n",
    "\n",
    "from threading import Thread\n",
    "import threading\n",
    "import ctypes\n",
    "class Thread_with_exception(Thread):\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs={}, Verbose=None , daemon=False):\n",
    "        Thread.__init__(self, group, target, name, args, kwargs , daemon=daemon)\n",
    "        \n",
    "        self._return = None\n",
    "            \n",
    "    def run(self):\n",
    "        if self._target is not None:\n",
    "            self._return = self._target(*self._args,\n",
    "                                                **self._kwargs)\n",
    "         \n",
    "    def get_id(self):\n",
    "        # returns id of the respective thread\n",
    "        if hasattr(self, '_thread_id'):\n",
    "            return self._thread_id\n",
    "        for id, thread in threading._active.items():\n",
    "            if thread is self:\n",
    "                return id\n",
    "\n",
    "    def join(self, *args):\n",
    "        Thread.join(self, *args)\n",
    "        return self._return\n",
    " \n",
    "    def raise_exception(self):\n",
    "        thread_id = self.get_id()\n",
    "        res = ctypes.pythonapi.PyThreadState_SetAsyncExc(thread_id, ctypes.py_object(SystemExit))\n",
    "        if res > 1:\n",
    "            ctypes.pythonapi.PyThreadState_SetAsyncExc(thread_id, 0)\n",
    "            print('Exception raise failure')\n",
    "            \n",
    "\n",
    "import time\n",
    "from utils.post_process import result_eq\n",
    "\n",
    "def is_queries_equal(testing_query , ground_truth_query , db_id , db_dir, timeout_time , gt_results = None):\n",
    "    #Input:\n",
    "        #time_out_time: integer: time in seconds\n",
    "    \n",
    "    if ground_truth_query!='':\n",
    "        with open('./log.txt', 'a') as f:\n",
    "            f.write(f\"procceing the ground_truth_query:\\n{ground_truth_query}\\n\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        gt_flag, gt_denotation = query_to_db(ground_truth_query , db_id , db_dir)\n",
    "    \n",
    "        gt_process_time = time.time() - start_time\n",
    "        with open('./log.txt', 'a') as f:\n",
    "            f.write(f\"processing time: {gt_process_time}\\n\\n\")\n",
    "        gt_results = (gt_flag, gt_denotation , gt_process_time)\n",
    "        \n",
    "    else:\n",
    "        gt_flag, gt_denotation, gt_process_time = gt_results\n",
    "\n",
    "    max_timeout_time = max( timeout_time , 2*gt_process_time )\n",
    "    print('query max timeout: ' , max_timeout_time)\n",
    "\n",
    "    test_thread = Thread_with_exception(target= query_to_db , args = (testing_query , db_id , db_dir ) , daemon=True )\n",
    "    with open('./log.txt', 'a') as f:\n",
    "        f.write(f\"proccessing the testing_query:\\n{testing_query}\\n\")\n",
    "    start_time = time.time()\n",
    "    test_thread.start()\n",
    "    test_result = test_thread.join(max_timeout_time)\n",
    "    if test_thread.is_alive():\n",
    "        with open('./log.txt', 'a') as f:\n",
    "            f.write(f\"**********processing is terminated due to timeout. max_timeout_time: {max_timeout_time}\\n\")\n",
    "        test_thread.raise_exception()\n",
    "        test_thread.join()\n",
    "        return False, gt_results\n",
    "        \n",
    "    test_flag, test_denotation = test_result\n",
    "    with open('./log.txt', 'a') as f:\n",
    "        f.write(f\"Processing time for the testing query: {time.time() - start_time}\\n\\n\")\n",
    "    \n",
    "    if gt_flag[0] != 'result':\n",
    "        with open('./log.txt', 'a') as f:\n",
    "            f.write(f\"!!!!!!!!!!The following ground truth has an error:\\n{ground_truth_query}\\n\\n\")\n",
    "        return False, gt_results\n",
    "    elif test_flag[0] != 'result':\n",
    "        return False, gt_results\n",
    "    elif 'ORDER BY' in ground_truth_query or 'order by' in ground_truth_query:\n",
    "        is_equal = result_eq(gt_flag[1] , test_flag[1] , order_matters=True)\n",
    "    else:\n",
    "        is_equal = result_eq(gt_flag[1] , test_flag[1] , order_matters=False)\n",
    "    \n",
    "    return is_equal, gt_results\n",
    "    \n",
    "import os\n",
    "def put_responses_back_to_json_dataset(index , json_dataset , sequences, dataset_type='spider' , timeout_time=1):#dataset_type=spider/bird\n",
    "    #Inputs:\n",
    "        #sequences: List of [ { 'generated_text' : gen_text } ] or [gen_text1 , gen_text2 , ...]\n",
    "        #json_dataset: json dataset that contains the ground truth in its 'response' part\n",
    "            #If the elements in input sequences are like seq[0]['generated_text'] then json_dataset should contain the prompt in 'prompts' part\n",
    "    gt_result_cache_file = './cache/' + dataset_type + '_results.pkl'\n",
    "    gt_results_is_cached = False\n",
    "    gt_results_list = []\n",
    "    \n",
    "    if os.path.isfile(gt_result_cache_file):\n",
    "        gt_results_is_cached = True\n",
    "        with open(gt_result_cache_file , 'rb' )as f:\n",
    "            gt_results_list = pkl.load(f)\n",
    "\n",
    "    db_dir = './DAIL-SQL/dataset/'+ dataset_type +'/database'\n",
    "    execution_accuracy = 0\n",
    "    \n",
    "    for i in range( 0, len(sequences), 1 ):\n",
    "        seq = sequences[i]\n",
    "        # print(f\"Number of processed sequences: {i}\\t|\\tNumber of correct queries: {execution_accuracy} \\n\")\n",
    "        # if i%10==0:\n",
    "        with open('./log.txt', 'a') as f:\n",
    "            f.write(f\"Number of processed sequences: {i}\\t|\\tNumber of correct queries: {execution_accuracy} \\n\")\n",
    "            \n",
    "        prompt_len = len ( json_dataset['questions'][index+i]['prompt'] )\n",
    "        if isinstance(seq, list):\n",
    "            gen_text = seq[0]['generated_text'][prompt_len:]\n",
    "        else:\n",
    "            # gen_text = seq[prompt_len:]\n",
    "            gen_text = seq\n",
    "            \n",
    "        processed_gen_text = post_process_get_sql_from_gentext(gen_text)\n",
    "        \n",
    "        db_id = json_dataset['questions'][index+i]['db_id']\n",
    "\n",
    "        # if dataset_type=='spider':\n",
    "\n",
    "        #     if gt_results_is_cached:\n",
    "        #         is_equal , gt_results = is_queries_equal(processed_gen_text , '' , db_id , db_dir, timeout_time , gt_results = gt_results_list[i])\n",
    "        #     else:\n",
    "        #         ground_truth = post_process_get_sql_from_gentext( json_dataset['questions'][index+i]['response'] )\n",
    "        #         is_equal , gt_results = is_queries_equal(processed_gen_text , ground_truth , db_id , db_dir, timeout_time )\n",
    "        #         gt_results_list.append(gt_results)\n",
    "        #     execution_accuracy += is_equal\n",
    "            \n",
    "        json_dataset['questions'][index+i]['response'] = processed_gen_text\n",
    "        # print('is_equal: ', is_equal)\n",
    "        # print('--------------------------')\n",
    "    if not gt_results_is_cached:\n",
    "        with open(gt_result_cache_file , 'wb' )as f:\n",
    "            pkl.dump( gt_results_list , f )\n",
    "        \n",
    "    return execution_accuracy\n",
    "\n",
    "from utils.post_process import process_duplication\n",
    "def post_process_get_sql_from_gentext(gen_text):\n",
    "    # remove \\n and extra spaces\n",
    "    # print(gen_text)\n",
    "    sql = \" \".join(gen_text.replace(\"\\n\", \" \").split())\n",
    "    sql = process_duplication(sql)\n",
    "    # python version should >= 3.8\n",
    "    if sql.startswith(\"SELECT\"):\n",
    "        sql = sql\n",
    "    elif sql.startswith(\" \"):\n",
    "        sql = \"SELECT\" + sql\n",
    "    else:\n",
    "        sql = \"SELECT \" + sql\n",
    "    return sql\n",
    "\n",
    "data_size=1034\n",
    "def eval_list_sql(sql_list , groundtruth_json_file_name , output_filename='' , dataset_type='spider' ):\n",
    "    #This function gets a list of sql predictions and evalueates it and creates a dail style dataset if the output_file_name is given\n",
    "    # groundtruth_json_file_name: Address of a dail style dataset containing the groundtruth in its response part.\n",
    "        #If the sql_list elements are of shape seq[0]['generated_text'] groundtruth_json_file_name should have the prompts too\n",
    "    with open(groundtruth_json_file_name , 'r') as f:\n",
    "        generated_prompts_file_byte = f.read()\n",
    "        generated_prompts = json.loads(generated_prompts_file_byte)\n",
    "        exec_acc = put_responses_back_to_json_dataset( 0 , generated_prompts , sql_list , dataset_type=dataset_type )\n",
    "    print('execution accuracy = ' , exec_acc/data_size)\n",
    "    with open('./log.txt', 'a') as f:\n",
    "        f.write(f\"execution accuracy = {exec_acc/data_size} \\n\")\n",
    "    if output_filename !='':\n",
    "        with open( output_filename , 'w' )as f:\n",
    "            json.dump(generated_prompts , f)\n",
    "\n",
    "def dail_dataset_to_response_list( dail_style_dataset_name ):\n",
    "    #This function gets a dail-style dataset and returns the all the responses in that dataset in a list.\n",
    "    return_list = []\n",
    "    with open( dail_style_dataset_name , 'r') as f:\n",
    "        generated_response_file_byte = f.read()\n",
    "        generated_response = json.loads(generated_response_file_byte)\n",
    "    for i in generated_response['questions']:\n",
    "        return_list.append( i['response'] )\n",
    "    return return_list\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0946f24a-1343-4624-87f9-5036fc122d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution accuracy =  0.42649903288201163\n"
     ]
    }
   ],
   "source": [
    "#re-evaluating a dataset that is created before\n",
    "import json\n",
    "groundtruth_json_file_name = './DAIL-SQL/dataset/process/SPIDER-TEST_SQL_0-SHOT_CTX-200_ANS-2048/questions.json'\n",
    "\n",
    "evaluating_dataset = './llama_pred/SPIDER_beam_4_lenpen0-MBRdbScore-TEST_SQL_3-SHOT-5Components_ranks0_4_8_12_16_EUCDISMASKPRESKLSIMTHR_QA-EXAMPLE_CTX-200_ANS-2048_llama_7b_v2.json'\n",
    "\n",
    "output_sequences = dail_dataset_to_response_list( evaluating_dataset )\n",
    "\n",
    "eval_list_sql(output_sequences , groundtruth_json_file_name , output_filename='' , dataset_type='spider' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0fc11f3d-c76f-4707-bfb8-e72beb0eb57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution accuracy =  0.0\n"
     ]
    }
   ],
   "source": [
    "#evaluating some generated prompts recently generated. This is for SPIDER\n",
    "import pickle as pkl\n",
    "\n",
    "groundtruth_json_file_name = './DAIL-SQL/dataset/process/SPIDER-TEST_SQL_3-SHOT_EUCDISMASKPRESKLSIMTHR_QA-EXAMPLE_CTX-200_ANS-2048_llama_7b/questions.json'\n",
    "\n",
    "# with open('./output_sequences-5_lenpen1.pkl', 'rb') as f:  # open a text file\n",
    "#     output_sequences = pkl.load(f)\n",
    "\n",
    "eval_list_sql(output_sequences , groundtruth_json_file_name , \n",
    "              output_filename='./llama_pred/SPIDER_beam_4_lenpen0-TEST_SQL_seq_level_TreeBased_regBeamSearch_Qwen7b_EUCDISMASKPRESKLSIMTHR_QA-EXAMPLE_CTX-200_ANS-2048_llama_7b.json' , dataset_type='spider' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3bfa9d98-9df6-4e10-b136-85b556ba5925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution accuracy =  0.0\n"
     ]
    }
   ],
   "source": [
    "#evaluating some generated prompts recently generated. This is for BIRD\n",
    "import pickle as pkl\n",
    "\n",
    "groundtruth_json_file_name = './DAIL-SQL/dataset/process/BIRD-TEST_SQL_0-SHOT_CTX-200_ANS-2048/questions.json'\n",
    "\n",
    "# with open('./output_sequences-5_lenpen1.pkl', 'rb') as f:  # open a text file\n",
    "#     output_sequences = pkl.load(f)\n",
    "\n",
    "eval_list_sql(output_sequences , groundtruth_json_file_name , \n",
    "              output_filename='./Qwen_pred/OmniSQL-7b_greedy_BIRD_wEvidence_0shot_OmniORG_chatTemplate.json' , dataset_type='bird' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "54c529b8-170b-4de2-96ae-9b99e29c8211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1534\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "file_name = './final_output_CodeS_preprocess_ensemble.pkl'\n",
    "with open(file_name , 'rb') as f:\n",
    "    output_sequences = pkl.load(f)\n",
    "\n",
    "print(len(output_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f736283f-f1ab-4f2d-9873-c740c6b1a656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./outputs1/output_sequences_0_50-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_50_100-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_100_150-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_150_200-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_200_250-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_250_300-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_300_350-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_350_400-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_400_450-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_450_500-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_500_550-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_550_600-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_600_650-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_650_700-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_700_750-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_750_800-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_800_850-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_850_900-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_900_950-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_950_1000-4.pkl\n",
      "1000\n",
      "./outputs1/output_sequences_1000_end-4.pkl\n",
      "680\n",
      "20680\n"
     ]
    }
   ],
   "source": [
    "#grouping the pieces of output_sequences\n",
    "import pickle as pkl\n",
    "prefix = './outputs1/output_sequences_'\n",
    "output_sequences = []\n",
    "for i in range(0 , 1050 , 50):\n",
    "    if i <1000:\n",
    "        if i == 0: \n",
    "            file_name = prefix + '0_'  + str(i+50).lstrip('0') + '-4.pkl'\n",
    "        else:\n",
    "            file_name = prefix + str(i).lstrip('0') + '_' + str(i+50).lstrip('0') + '-4.pkl'\n",
    "    else:\n",
    "        file_name = prefix + str(i).lstrip('0') + '_end' + '-4.pkl'\n",
    "    print(file_name)\n",
    "    with open(file_name , 'rb') as f:\n",
    "        part_of_output = pkl.load(f)\n",
    "        print(len(part_of_output))\n",
    "        # new_part_of_output = []\n",
    "        # if len(part_of_output)==170:\n",
    "        #     for x in range(0,len(part_of_output),5):\n",
    "        #         new_part_of_output.append(x)\n",
    "        #     with open('./new_'+file_name[2:], 'wb')as h:\n",
    "        #         pkl.dump(new_part_of_output , h)\n",
    "    output_sequences.extend(part_of_output)\n",
    "\n",
    "print( len(output_sequences) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "43f1dae8-9594-42e3-bc12-6a429a9a0a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! You have successfully answered the following questions. You are very knowledgeable about Databases and SQL.\n",
      "<|endoftext|>database schema :\n",
      "table staff , columns = [ staff.email ( text | comment : email of the staff ) , staff.first_name ( text | values : Mike , Jon ) , staff.password ( text ) , staff.address_id ( integer | values : 3 , 4 ) , staff.last_update ( datetime | values : 2006-02-15 04:57:16.0 ) , staff.store_id ( integer | values : 1 , 2 ) , staff.picture ( blob | comment : picture of the staff ) , staff.username ( text | values : Mike , Jon ) , staff.staff_id ( integer | primary key | values : 1 , 2 ) , staff.active ( integer | values : 1 ) ]\n",
      "table film_actor , columns = [ film_actor.actor_id ( integer | primary key | values : 1 , 2 ) , film_actor.film_id ( integer | primary key | values : 1 , 23 ) , film_actor.last_update ( datetime | values : 2006-02-15 05:05:03.0 ) ]\n",
      "table language , columns = [ language.language_id ( integer | primary key | values : 1 , 2 ) , language.name ( text | values : English , Italian ) , language.last_update ( datetime | values : 2006-02-15 05:02:19.0 ) ]\n",
      "table film , columns = [ film.title ( text | values : ACADEMY DINOSAUR , ACE GOLDFINGER ) , film.language_id ( integer | values : 1 ) , film.last_update ( datetime | values : 2006-02-15 05:03:42.0 ) , film.replacement_cost ( real | values : 20.99 , 12.99 ) , film.special_features ( text | values : Trailers,Deleted Scenes ) , film.original_language_id ( integer ) , film.rating ( text | values : PG , G ) , film.rental_duration ( integer | values : 6 , 3 ) , film.film_id ( integer | primary key | values : 1 , 2 ) , film.release_year ( text | values : 2006 ) ]\n",
      "table actor , columns = [ actor.last_update ( datetime | values : 2006-02-15 04:34:33.0 ) , actor.actor_id ( integer | primary key | values : 1 , 2 ) , actor.last_name ( text | values : GUINESS , WAHLBERG ) , actor.first_name ( text | values : PENELOPE , NICK ) ]\n",
      "table rental , columns = [ rental.last_update ( datetime | values : 2006-02-15 21:30:53.0 , 2006-02-23 04:12:08.0 ) , rental.return_date ( datetime | values : 2005-05-26 22:04:30.0 , 2005-05-28 19:40:33.0 ) , rental.inventory_id ( integer | values : 367 , 1525 ) , rental.customer_id ( integer | values : 130 , 459 ) , rental.rental_id ( integer | primary key | values : 1 , 2 ) , rental.staff_id ( integer | values : 1 , 2 ) , rental.rental_date ( datetime | values : 2005-05-24 22:53:30.0 , 2005-05-24 22:54:33.0 ) ]\n",
      "foreign keys : None\n",
      "matched contents :\n",
      "language.name ( English )\n",
      "film.rating ( NC-17 )\n",
      "English is a name of a language; for adults only refers to rating = 'NC-17'; How many films in English are for adults only?\n",
      "SELECT count(film.film_id) FROM film INNER JOIN LANGUAGE ON film.language_id = LANGUAGE.language_id WHERE LANGUAGE.name = 'English' AND film.rating = 'NC-17' ;\n",
      "\n",
      "\n",
      "\n",
      "<|endoftext|>database schema :\n",
      "table events , columns = [ events.longitude ( real | values : 121.0 , 104.0 ) , events.latitude ( real | values : 31.0 , 30.0 ) , events.device_id ( integer | values : 29182687948017175 , -6401643145415154744 ) , events.timestamp ( datetime | values : 2016-05-01 00:55:25.0 , 2016-05-01 00:54:12.0 ) , events.event_id ( integer | primary key | values : 1 , 2 ) ]\n",
      "table gender_age_train , columns = [ gender_age_train.age ( integer | values : 24 , 36 ) , gender_age_train.gender ( text | values : M , F ) , gender_age_train.group ( text | values : M23-26 , M32-38 ) , gender_age_train.device_id ( integer | primary key | values : -9223067244542181226 , -9222956879900151005 ) ]\n",
      "table sample_submission , columns = [ sample_submission.`m22-` ( real | values : 0.0833 ) , sample_submission.`m29-31` ( real | values : 0.0833 ) , sample_submission.`f27-28` ( real | values : 0.0833 ) , sample_submission.device_id ( integer | primary key | values : -9223321966609553846 , -9223042152723782980 ) , sample_submission.`m23-26` ( real | values : 0.0833 ) , sample_submission.`f23-` ( real | values : 0.0833 ) , sample_submission.`f24-26` ( real | values : 0.0833 ) , sample_submission.`f33-42` ( real | values : 0.0833 ) , sample_submission.`m27-28` ( real | values : 0.0833 ) , sample_submission.`m32-38` ( real | values : 0.0833 ) ]\n",
      "table gender_age_test , columns = [ gender_age_test.device_id ( integer | primary key | values : -9223321966609553846 , -9223042152723782980 ) ]\n",
      "table phone_brand_device_model2 , columns = [ phone_brand_device_model2.device_model ( text | primary key | values : 红米note , Y19T ) , phone_brand_device_model2.device_id ( integer | primary key | values : -9223321966609553846 , -9223067244542181226 ) , phone_brand_device_model2.phone_brand ( text | primary key | values : 小米 , vivo ) ]\n",
      "table app_events_relevant , columns = [ app_events_relevant.event_id ( integer | primary key | values : 2 , 6 ) , app_events_relevant.app_id ( integer | primary key | values : -8942695423876075857 , -8022267440849930066 ) , app_events_relevant.is_active ( integer | values : 0 , 1 ) , app_events_relevant.is_installed ( integer | values : 1 ) ]\n",
      "foreign keys : None\n",
      "matched contents :\n",
      "phone_brand_device_model2.phone_brand ( vivo )\n",
      "event no. refers to event_id; event_id = 2; vivo devices refers to phone_brand = 'vivo'; Among the devices with event no.2 happening, how many of them are vivo devices?\n",
      "SELECT count(phone_brand_device_model2.device_id) FROM phone_brand_device_model2 INNER JOIN EVENTS ON EVENTS.device_id = phone_brand_device_model2.device_id WHERE phone_brand_device_model2.phone_brand = 'vivo' AND EVENTS.event_id = 2 ;\n",
      "\n",
      "\n",
      "<|endoftext|>database schema :\n",
      "table schools , columns = [ schools.virtual ( text | values : P , N ) , schools.edopscode ( text | comment : education option code | values : TRAD , JUV ) , schools.cdscode ( text | primary key | values : 01100170000000 , 01100170109835 ) , schools.school ( text | values : FAME Public Charter ) , schools.charternum ( text | values : 0728 , 0811 ) , schools.ncesdist ( text | comment : national center for educational statistics school district identification number | values : 0691051 , 0600002 ) , schools.edopsname ( text | comment : educational option name | values : Traditional , Juvenile Court School ) , schools.charter ( integer | values : 1 , 0 ) , schools.district ( text ) , schools.ncesschool ( text | comment : national center for educational statistics school identification number | values : 10546 , 10947 ) ]\n",
      "table satscores , columns = [ satscores.cds ( text | primary key | values : 10101080000000 , 10101080109991 ) , satscores.sname ( text | comment : school name | values : FAME Public Charter ) , satscores.numtsttakr ( integer | comment : number of test takers | values : 88 , 17 ) , satscores.dname ( text | comment : district name | values : Alameda Unified ) , satscores.rtype ( text | values : D , S ) , satscores.avgscrmath ( integer | comment : average scores in math | values : 418 , 546 ) , satscores.numge1500 ( integer | comment : number of test takers whose total sat scores are greater or equal to 1500 | values : 14 , 9 ) , satscores.cname ( text | comment : county name | values : Alameda , Amador ) , satscores.avgscrread ( integer | comment : average scores in reading | values : 418 , 503 ) , satscores.enroll12 ( integer | comment : enrollment (1st-12nd grade) | values : 398 , 62 ) ]\n",
      "table frpm , columns = [ frpm.`school name` ( text | values : FAME Public Charter ) , frpm.cdscode ( text | primary key | values : 01100170109835 , 01100170112607 ) , frpm.`school code` ( text | values : 0109835 , 0112607 ) , frpm.`district name` ( text ) , frpm.`district code` ( integer | values : 10017 , 31609 ) , frpm.`school type` ( text | values : K-12 Schools (Public) , High Schools (Public) ) , frpm.`county name` ( text | values : Alameda , Alpine ) , frpm.`district type` ( text | values : State Special Schools ) , frpm.`county code` ( text | values : 01 , 02 ) , frpm.irc ( integer | values : 1 , 0 ) ]\n",
      "foreign keys :\n",
      "frpm.cdscode = schools.cdscode\n",
      "satscores.cds = schools.cdscode\n",
      "matched contents :\n",
      "satscores.numtsttakr ( 400 )\n",
      "satscores.avgscrmath ( 400 )\n",
      "satscores.avgscrread ( 400 )\n",
      "Exclusively virtual refers to virtual = 'F'; How many schools with an average score in Math under 400 in the SAT test are exclusively virtual?\n",
      "SELECT\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "with open('./vulcan_output/output_CodeS_beam4_bird_wEvidence_2shot_SFTCodeS-7b_2ndOf0-5shot_EOSbefore_batch_text.pkl', 'rb') as f:\n",
    "    batch_text = pkl.load(f)\n",
    "x = 5\n",
    "print(batch_text[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "0a8242de-f144-4f52-ab49-cf7e908f0d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>database schema : table products , columns = [ products.productid ( integer | primary key | values : 1 , 2 ) , products.description ( text | values : Rucní zadání , Nafta ) ]\n",
      "table yearmonth , columns = [ yearmonth.consumption ( real | values : 528.3 , 1598.28 ) , yearmonth.date ( text | primary key | values : 201112 , 201201 ) , yearmonth.customerid ( integer | primary key | values : 39 , 63 ) ]\n",
      "table transactions_1k , columns = [ transactions_1k.productid ( integer | values : 2 , 23 ) , transactions_1k.date ( date | values : 2012-08-24 , 2012-08-23 ) , transactions_1k.customerid ( integer | values : 31543 , 46707 ) , transactions_1k.transactionid ( integer | primary key | values : 1 , 2 ) , transactions_1k.time ( text | values : 09:41:00 , 10:03:00 ) , transactions_1k.cardid ( integer | values : 486621 , 550134 ) , transactions_1k.price ( real | values : 672.64 , 430.72 ) , transactions_1k.amount ( integer | values : 28 , 18 ) , transactions_1k.gasstationid ( integer | values : 3704 , 656 ) ]\n",
      "table customers , columns = [ customers.customerid ( integer | primary key | values : 3 , 5 ) , customers.currency ( text | values : EUR , CZK ) , customers.segment ( text | comment : client segment | values : SME , LAM ) ]\n",
      "table gasstations , columns = [ gasstations.country ( text | values : CZE , SVK ) , gasstations.chainid ( integer | values : 13 , 6 ) , gasstations.gasstationid ( integer | primary key | values : 44 , 45 ) , gasstations.segment ( text | comment : chain segment | values : Value for money , Premium ) ]\n",
      "foreign keys : None\n",
      "matched contents :\n",
      "products.productid ( 4 )\n",
      "yearmonth.date ( 201309 )\n",
      "transactions_1k.transactionid ( 4 )\n",
      "transactions_1k.amount ( 4 )\n",
      "gasstations.chainid ( 4 )\n",
      "gasstations.gasstationid ( 2013 )\n",
      "September 2013 refers to 201309; First 4 strings represent the year; Please list the product description of the products consumed in September, 2013.\n"
     ]
    }
   ],
   "source": [
    "with open('./vulcan_output/output_CodeS_beam4_bird_wEvidence_0shotFrom0shot_SFTCodeS-7b_padTrue_batch_text.pkl', 'rb') as f:\n",
    "    batch_text = pkl.load(f)\n",
    "\n",
    "print(batch_text[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a879a8de-b1a6-4ef2-a09a-3e3c6b61954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg number of distinced candidates: 9.132333767926989\n"
     ]
    }
   ],
   "source": [
    "with open('MBRoutput_sequences_total-4.pkl' , 'rb') as f:\n",
    "    seqs = pkl.load(f)\n",
    "\n",
    "total_distinced_candid = 0\n",
    "for i in seqs:\n",
    "    my_dict = dict()\n",
    "    for t in i:\n",
    "        for c in t:\n",
    "            my_dict[c] = 1\n",
    "    num_dist_candid = len(my_dict.keys())\n",
    "    total_distinced_candid+=num_dist_candid\n",
    "\n",
    "print(f'avg number of distinced candidates: {total_distinced_candid/len(seqs)}')\n",
    "# print(len(seqs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
