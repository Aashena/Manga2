{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "604f812d-d489-4c06-8fbd-b251eca43924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for sequence-level-voting ensemble using my designed scoring mechanism\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import AutoTokenizer\n",
    "from collections import defaultdict , Counter\n",
    "import numpy as np\n",
    "import math\n",
    "import CodeS.SQLtree_based_similarity as sql_scoring\n",
    "import copy\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# MODEL_NAME = 'huggyllama/llama-7b'\n",
    "# MODEL_NAME = 'seeklhy/codes-1b'\n",
    "MODEL_NAME = 'seeklhy/codes-7b-bird-with-evidence'\n",
    "# MODEL_NAME = 'Qwen/Qwen2.5-Coder-7B-Instruct'\n",
    "# MODEL_NAME = 'Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int8'\n",
    "\n",
    "\n",
    "NUMBER_of_components = int(8) #Don't Forget to set this peoperly!!!!\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# TOKENIZER.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# TOKENIZER.padding_side=\"left\"\n",
    "# LENGTH_PEN = 0.1\n",
    "LENGTH_PEN = 1\n",
    "\n",
    "def group_tokenized_responses( tokenized_responses, inputs_log_prob):\n",
    "        # Dictionary to group indices of identical tokenized responses\n",
    "        groups = defaultdict(list)\n",
    "\n",
    "        # Iterate over the tokenized responses with their indices\n",
    "        for i, tokens in enumerate(tokenized_responses):\n",
    "            # Convert tokens to a tuple (hashable) to use as a dictionary key\n",
    "            token_bag_with_counts = tuple(sorted(Counter(tokens).items()))\n",
    "            groups[token_bag_with_counts].append(i)\n",
    "\n",
    "        # print('groups.keys(): ', groups.keys())\n",
    "        # print('groups: ' , groups)\n",
    "        # Convert the grouped dictionary values into a list of groups\n",
    "        grouped_indices = list(groups.values())\n",
    "\n",
    "        # Calculate the average log probabilities for each group and assign them\n",
    "        for group in grouped_indices:\n",
    "            # Extract log probabilities for the current group\n",
    "            group_probs = [\n",
    "                inputs_log_prob[int(idx % NUMBER_of_components)][ 0, int(idx / NUMBER_of_components)].item()\n",
    "                for idx in group\n",
    "            ]\n",
    "\n",
    "            # Calculate the average probability for the group\n",
    "            avg_prob = np.mean(group_probs)\n",
    "\n",
    "            # Assign the average probability back to each index in the group\n",
    "            for idx in group:\n",
    "                inputs_log_prob[int(idx % NUMBER_of_components)][ 0, int(idx / NUMBER_of_components)] = avg_prob\n",
    "\n",
    "        return grouped_indices, inputs_log_prob\n",
    "\n",
    "def custom_tokenize( text, must_have_textnum=True):\n",
    "    # Regular expression pattern to match quoted text\n",
    "    quote_pattern = r'\\\".*?\\\"|\\`.*?\\`|\\'.*?\\''\n",
    "    \n",
    "    # Find all quoted texts\n",
    "    quoted_texts = nltk.re.findall(quote_pattern, text)\n",
    "\n",
    "    # Replace quoted texts with placeholders to avoid re-tokenization\n",
    "    placeholder = \"QUOTE_PLACE_HOLDER\"\n",
    "    modified_text = nltk.re.sub(quote_pattern, placeholder, text)\n",
    "\n",
    "    # Tokenize the modified text using word_tokenize\n",
    "    tokens = word_tokenize(modified_text)\n",
    "\n",
    "    # Replace placeholders with the original quoted texts\n",
    "    final_tokens = []\n",
    "    quote_index = 0\n",
    "    for token in tokens:\n",
    "        if token == placeholder:\n",
    "            final_tokens.append(quoted_texts[quote_index])\n",
    "            quote_index += 1\n",
    "        else:\n",
    "            final_tokens.append(token)\n",
    "\n",
    "    return [s for s in final_tokens if re.search(r'[a-zA-Z0-9]', s)] if must_have_textnum else final_tokens\n",
    "\n",
    "#calculating the similarity score of the two tokenized sentences.\n",
    "def similarity_meassure( hypothesis_tokens , reference_tokens, in_table_keywords):\n",
    "    # hypothesis_tokens = [token for token in hypothesis_tokens if token.upper() not in self.sql_keywords]\n",
    "    # reference_tokens = [token for token in reference_tokens if token.upper() not in self.sql_keywords]\n",
    "\n",
    "    hypothesis_counts = Counter(hypothesis_tokens)\n",
    "    reference_counts = Counter(reference_tokens)\n",
    "    clipped_counts = dict()\n",
    "\n",
    "    for token in hypothesis_tokens:\n",
    "        if token not in reference_counts.keys():\n",
    "            reference_counts[token] = 0\n",
    "        if token in in_table_keywords:\n",
    "            clipped_counts[token] = min(hypothesis_counts[token], reference_counts[token])\n",
    "            # print(f'{token}')\n",
    "        else:\n",
    "            clipped_counts[token] = min(hypothesis_counts[token], reference_counts[token])/2\n",
    "    \n",
    "    total_clipped = sum(clipped_counts.values())\n",
    "\n",
    "    similarity = (total_clipped*2+0.1) / ( len(hypothesis_tokens) + len(reference_tokens) + 0.1 )\n",
    "\n",
    "    return similarity\n",
    "\n",
    "def decode_tokenize_apply_lenpen(inputs_ids, inputs_log_prob, starting_batch_input_len):\n",
    "    tokenized_responses = []\n",
    "    decoded_text_list = []\n",
    "    tmp_input_log_prob = []\n",
    "    #Tokenizing the candidate sequences\n",
    "    for comp_input_ids,comp_starting_tok in zip(inputs_ids,starting_batch_input_len):\n",
    "        decoded_text_list.extend( TOKENIZER.batch_decode( comp_input_ids[ comp_starting_tok: , : ].transpose(0,1) ,\n",
    "                                    skip_special_tokens=True ) )\n",
    "    decoded_text_list_1 = [] # This is for SPIDER dataset and llama model\n",
    "    for text in decoded_text_list: #number of candidates we have for each question\n",
    "        # print(text)\n",
    "        text = post_process_get_sql_from_gentext(text) # This is for SPIDER dataset and llama model\n",
    "        decoded_text_list_1.append(text)\n",
    "        tokenized_responses.append( custom_tokenize( text.replace('.' , ' ')))#, must_have_textnum=False ) )\n",
    "    decoded_text_list = decoded_text_list_1 # This is for SPIDER dataset and llama model\n",
    "\n",
    "    #Finding the identical candidates, take the average of their probability, and only keep one of them with the average probability assigned to it.\n",
    "    # grouped_indices, inputs_log_prob = group_tokenized_responses( tokenized_responses, inputs_log_prob)\n",
    "    for comp_inputs_log_prob, comp_input_ids, comp_starting_tok in zip(inputs_log_prob, inputs_ids, starting_batch_input_len):\n",
    "        gen_text_len = comp_input_ids.size(0) - comp_starting_tok\n",
    "        # print('comp_input_ids.size(0): ' , comp_input_ids.size(0))\n",
    "        # print('comp_starting_tok: ' , comp_starting_tok)\n",
    "        # print(comp_inputs_log_prob)\n",
    "        # print((gen_text_len** LENGTH_PEN ))\n",
    "        tmp_input_log_prob.append(comp_inputs_log_prob/(gen_text_len** LENGTH_PEN ))\n",
    "    return tokenized_responses, decoded_text_list, tmp_input_log_prob\n",
    "\n",
    "from utils.post_process import process_duplication\n",
    "def post_process_get_sql_from_gentext(gen_text):\n",
    "    # remove \\n and extra spaces\n",
    "    # print(gen_text)\n",
    "    sql = \" \".join(gen_text.replace(\"\\n\", \" \").split())\n",
    "    sql = process_duplication(sql)\n",
    "    # python version should >= 3.8\n",
    "    if sql.startswith(\"SELECT\"):\n",
    "        sql = sql\n",
    "    elif sql.startswith(\" \"):\n",
    "        sql = \"SELECT\" + sql\n",
    "    else:\n",
    "        sql = \"SELECT \" + sql\n",
    "    return sql\n",
    "\n",
    "def scale_to_0_and_1(arr):\n",
    "    # Min-max scaling\n",
    "    arr_min = np.min(arr)\n",
    "    arr_max = np.max(arr)\n",
    "    scaled = (arr - arr_min) / (arr_max - arr_min)\n",
    "    return scaled\n",
    "    \n",
    "def ensemble( inputs_ids , inputs_log_prob , batch_text , starting_batch_input_len=None, similarity_func='avg' , keep_all=False, comp_prob=None ):# , past_key_value_tensor ):\n",
    "    #function for performing ensemble using the bleu metric between the candidate sequences.\n",
    "    #input:\n",
    "        #inputs_ids: torch tensor representing the prompt tokens per component with shape (batch_size , input_len , num_beam)\n",
    "        #inputs_log_prob: torch tensor representing the probability of each input_ids with the shape(batch_size , 1 , num_beam)\n",
    "        #starting_batch_input_len: The length of the batch before any predictions.\n",
    "        #extra_added_paddings: torch tensor with size (batch_size , 1 , num_beam) recording the token index of the last token of the prompt (where the answer starts)\n",
    "        #batch_text: list of string with len()=number_components. Having the prompts for each components.\n",
    "    #return:\n",
    "        #ensembled_inputs_ids: torch tensor with size (batch_size , input_len , num_beam)\n",
    "        #ensembled_inputs_log_prob: torch tensor with size (batch_size , 1 , num_beam)\n",
    "    \n",
    "    if comp_prob is None:\n",
    "        comp_prob = torch.tensor([0 for i in range(len(inputs_ids)) ])\n",
    "    batch_size = len(inputs_ids)\n",
    "    num_beam = inputs_ids[0].size( dim=-1 ) if starting_batch_input_len is not None else len(inputs_ids[0])\n",
    "    \n",
    "\n",
    "    table_creation_part_prompt = batch_text[0].split('database schema :')[-1] # this is for BIRD\n",
    "    # table_creation_part_prompt = batch_text[0].split('Given the following database schema:')[-1].split('Answer the following')[0]#This is for SPIDER\n",
    "    \n",
    "    in_table_keywords = custom_tokenize( table_creation_part_prompt.replace('.' , ' ')  )\n",
    "    # print('in_table_keywords: ' , in_table_keywords)\n",
    "    selection_score_list = [] #This scoring is used to select the best candidates. It uses the length penalty to calculate the scores\n",
    "\n",
    "    if starting_batch_input_len is None:\n",
    "        decoded_text_list = []\n",
    "        tokenized_responses = []\n",
    "        tmp_input_log_prob = inputs_log_prob\n",
    "        for comp_beams in inputs_ids:\n",
    "            post_processed_beams = []\n",
    "            for beam in comp_beams:\n",
    "                post_processed_beams.append(post_process_get_sql_from_gentext(beam))\n",
    "            decoded_text_list.extend(post_processed_beams)\n",
    "        for text in decoded_text_list: #number of candidates we have for each question\n",
    "            tokenized_responses.append( custom_tokenize( text.replace('.' , ' ')))#, must_have_textnum=False ) )\n",
    "        \n",
    "    else:\n",
    "        tokenized_responses, decoded_text_list, tmp_input_log_prob = decode_tokenize_apply_lenpen(inputs_ids, inputs_log_prob, starting_batch_input_len)\n",
    "\n",
    "    # print(f'tmp_input_log_prob: {tmp_input_log_prob}')\n",
    "    similarity_matrix = []\n",
    "    prob_matrix = []\n",
    "    prob_vector = []\n",
    "    #Calculating the score for each candidate\n",
    "    for j in range( len( tokenized_responses ) ): #[tok_component1_beam1, tok_component2_beam1, tok_component3_beam1, ..., tok_component1_beam2, tok_component2_beam2 , ...]\n",
    "        temp_tokenized_responses = copy.deepcopy( tokenized_responses )\n",
    "        temp_query_list = decoded_text_list.copy()\n",
    "        query = temp_query_list.pop(j)\n",
    "        tokenized_response = temp_tokenized_responses.pop(j)\n",
    "        selection_score = 0\n",
    "        selection_denominator = 0\n",
    "        similarity_list = []\n",
    "        prob_list = []\n",
    "        for index , (other_tokenized_response, other_query) in enumerate( zip( temp_tokenized_responses , temp_query_list ) ):\n",
    "            if index>=j:\n",
    "                index+=1\n",
    "            \n",
    "            if query=='':\n",
    "                query='***weirdanswer***'\n",
    "            elif other_query=='':\n",
    "                other_query='***weirdanswer***'\n",
    "            \n",
    "            jacc_weighted_sim = similarity_meassure( tokenized_response, other_tokenized_response, in_table_keywords )\n",
    "            \n",
    "            if similarity_func == 'jacc':\n",
    "                similarity = math.log(jacc_weighted_sim)\n",
    "            elif similarity_func == 'tree':\n",
    "                tree_based_sim = sql_scoring.unparsed_query_similarity(query , other_query)\n",
    "                if tree_based_sim == 0:\n",
    "                    similarity = math.log(jacc_weighted_sim)\n",
    "                else:\n",
    "                    similarity = math.log( tree_based_sim )\n",
    "            elif similarity_func == 'bleu':\n",
    "                blue_score = sentence_bleu( word_tokenize( query.replace('.' , ' ') ) , word_tokenize( other_query.replace('.' , ' ') ) , smoothing_function = SmoothingFunction().method1)\n",
    "                if blue_score == 0:\n",
    "                    similarity = math.log(jacc_weighted_sim)\n",
    "                else:\n",
    "                    similarity = math.log( blue_score )\n",
    "            # elif similarity_func=='avg':\n",
    "            #     if tree_based_sim == 0:\n",
    "            #         similarity = math.log(jacc_weighted_sim)\n",
    "            #     else:\n",
    "            #         similarity = math.log( (jacc_weighted_sim + tree_based_sim)/2 )\n",
    "            similarity_list.append(similarity)\n",
    "            prob_with_lenpen = tmp_input_log_prob[ int(index/num_beam)][ int(index%num_beam) ] + comp_prob[ int(index/num_beam) ]\n",
    "            prob_list.append(prob_with_lenpen)\n",
    "\n",
    "        prob_vector.append(tmp_input_log_prob[ int(j/num_beam) ] [ int(j%num_beam) ] + comp_prob[ int(j/num_beam) ])\n",
    "        similarity_matrix.append(similarity_list)\n",
    "        prob_matrix.append(prob_list)\n",
    "        \n",
    "    # print('similarity_matrix: ' , similarity_matrix)\n",
    "    scaled_similarity = scale_to_0_and_1( np.array(similarity_matrix) )\n",
    "    # print('scaled_similarity: ' , scaled_similarity)\n",
    "    sim_prob_matrix = np.array(prob_matrix) + scaled_similarity\n",
    "    # print('sim_prob_matrix: ' , sim_prob_matrix)\n",
    "    scaled_sim_prob = scale_to_0_and_1( sim_prob_matrix )\n",
    "    # print('scaled_sim_prob: ' , scaled_sim_prob)\n",
    "    scaled_prob_vector = scale_to_0_and_1( np.array(prob_vector) )\n",
    "\n",
    "    selection_score=None\n",
    "    for i in range(scaled_sim_prob.shape[1]):\n",
    "        if selection_score is None:\n",
    "            selection_score = scaled_sim_prob[:,0:1]\n",
    "        else:\n",
    "            new_comparison_vector = np.concatenate( (selection_score , scaled_sim_prob[:,i:i+1]) , axis=1 )\n",
    "            alpha = torch.from_numpy( np.max( new_comparison_vector , axis=1 ) ).unsqueeze(1)\n",
    "            beta = torch.from_numpy( np.min( new_comparison_vector , axis=1 ) ).unsqueeze(1)\n",
    "            selection_score = alpha + torch.log1p( torch.exp(beta-alpha) )\n",
    "\n",
    "    scaled_selection_score = scale_to_0_and_1( selection_score.detach().cpu().numpy() )\n",
    "    \n",
    "    # print( 'scaled_selection_score.shape: ' , scaled_selection_score.shape )\n",
    "    # print( 'scaled_prob_vector.shape: ' , scaled_prob_vector.shape )\n",
    "    \n",
    "    final_score = ( scaled_selection_score + np.expand_dims( scaled_prob_vector, 1 ) )\n",
    "    # print('final_score: ', final_score )\n",
    "        \n",
    "    selection_score_list = final_score.flatten().tolist()\n",
    "    \n",
    "    if keep_all==False:\n",
    "        #Ignoring all identical candidates and only keeping one.\n",
    "        for group in grouped_indices:\n",
    "            is_first_item = True\n",
    "            for index in group:\n",
    "                if is_first_item ==False:\n",
    "                    selection_score_list[index] = torch.finfo(torch.float32).min\n",
    "                    # selection_score_list[index] = -100000\n",
    "                else:\n",
    "                    is_first_item = False\n",
    "\n",
    "    output_queries_in_order = [] #containing tuples like (component_index , beam_index)\n",
    "    #Finding the sequence with the highest bleu score.\n",
    "    tmp_score_list = selection_score_list.copy()\n",
    "    if keep_all == True:\n",
    "        num_sel_candid = NUMBER_of_components * num_beam\n",
    "    else:\n",
    "        num_sel_candid = num_beam\n",
    "    for beam in range(num_sel_candid):\n",
    "        max_bleu_score_value = max( tmp_score_list )\n",
    "        # print('max_bleu_score_value: ', max_bleu_score_value)\n",
    "        # print('tmp_score_list: ' , tmp_score_list)\n",
    "        max_index_bleu_score = tmp_score_list.index(max_bleu_score_value)\n",
    "        # print(torch.finfo(torch.float32).min)\n",
    "        tmp_score_list[ max_index_bleu_score ] = torch.finfo(torch.float32).min\n",
    "        # tmp_score_list[ max_index_bleu_score ] = -100000\n",
    "        output_queries_in_order.append(  decoded_text_list[ max_index_bleu_score ]  )\n",
    "        \n",
    "    return output_queries_in_order\n",
    "\n",
    "#To use the above algorithm we need the following things: input_ids, inputs_log_prob, starting_batch_input_len, batch_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c0757e-a2fc-4ddd-be68-5632d9d9aea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n"
     ]
    }
   ],
   "source": [
    "#Performing ensemble on the list of lists that are provided from the cell below\n",
    "import pickle as pkl\n",
    "import math\n",
    "prefix = './MBRoutput_sequences_total-4'\n",
    "\n",
    "#This is for the time when we have input_ids and _starting_batch_input_len:\n",
    "# post_fixes = ['_batch_text' , '_input_ids' , '_inputs_log_prob' , '_starting_batch_input_len' ]\n",
    "# lists_of_parameter_list = [ [],[],[],[] ]\n",
    "\n",
    "##This is for the time when we only have the strings and the probabilities are already with length penalty\n",
    "post_fixes = ['_batch_text' , '_a1b1Joint6comp_beams_log_prob' , '']\n",
    "# post_fixes = ['_batch_text' , '_input_ids' , '']\n",
    "lists_of_parameter_list = [ [],[],[] ]\n",
    "comp_prob_filenames = ['oracle_comp_prob_per_sample_2.pkl' , 'oracle_comp_prob_per_sample_2.pkl' , 'oracle_comp_prob_per_sample_divpen05_1.pkl',\n",
    "                       'oracle_comp_prob_per_sample_divpen05_2.pkl' , 'oracle_comp_prob_per_sample_divpen1_1.pkl', 'oracle_comp_prob_per_sample_divpen1_2.pkl',\n",
    "                      'oracle_comp_prob_per_sample_divpen2_1.pkl', 'oracle_comp_prob_per_sample_divpen2_2.pkl'] #This is for component probability\n",
    "comp_prob_list = [  ] # This is for the component probability\n",
    "\n",
    "#reading the output files to get the parameter to perform ensemble.\n",
    "for index, post_fix in enumerate(post_fixes):\n",
    "    filename = prefix + post_fix + '.pkl'\n",
    "    with open(filename , 'rb') as f:\n",
    "        lists_of_parameter_list[index] = pkl.load(f)\n",
    "        \n",
    "for i , filename in enumerate(comp_prob_filenames): #This is for the component probability\n",
    "    with open(filename , 'rb') as f:\n",
    "        comp_prob_list.append(pkl.load(f))\n",
    "\n",
    "ordered_final_answers_list = []\n",
    "for sample_idx in range( len(lists_of_parameter_list[0]) ):\n",
    "    print(sample_idx)\n",
    "    if len(lists_of_parameter_list) ==3:\n",
    "        batch_text = lists_of_parameter_list[0][sample_idx]\n",
    "        inputs_log_prob = lists_of_parameter_list[1][sample_idx]\n",
    "        output_text = lists_of_parameter_list[2][sample_idx]\n",
    "        comps_probs = torch.tensor([ math.log(comp_prob_list[i][sample_idx]) for i in range(len(comp_prob_list))]) #This is for the component probability\n",
    "        ordered_final_answers = ensemble(output_text, inputs_log_prob, batch_text, similarity_func='tree' , keep_all=True, comp_prob=None)#comps_probs )\n",
    "    else:\n",
    "        batch_text = lists_of_parameter_list[0][sample_idx]\n",
    "        inputs_ids = lists_of_parameter_list[1][sample_idx]\n",
    "        inputs_log_prob = lists_of_parameter_list[2][sample_idx]\n",
    "        starting_batch_input_len = lists_of_parameter_list[3][sample_idx]\n",
    "        comps_probs = torch.tensor([ math.log(comp_prob_list[i][sample_idx]) for i in range(len(comp_prob_list))]) #This is for the component probability\n",
    "        ordered_final_answers = ensemble(inputs_ids, inputs_log_prob, batch_text,\n",
    "                                         starting_batch_input_len, similarity_func='tree' , keep_all=True, comp_prob=comps_probs )\n",
    "    ordered_final_answers_list.extend(ordered_final_answers)\n",
    "    # break\n",
    "\n",
    "final_output_filenames = 'output_1shot_diffDivpen_8in0-2shotComps_beam4_seq_mine_scaled_pureSeqlevelTreebased_SFTCodeS-7b_a1b1jointMargin6compProb.pkl'\n",
    "with open(final_output_filenames , 'wb')as f:\n",
    "    pkl.dump(ordered_final_answers_list , f)\n",
    "print('len(ordered_final_answers_list): ' , len(ordered_final_answers_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22b1e6b9-4861-4e27-b1cf-65f5619176c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_codeS_beam5_bird_lenpen05_0-5shot_OneSQLQwen-32b_batch_text.pkl: len=1534\n",
      "output_codeS_beam5_bird_lenpen05_0-5shot_OneSQLQwen-32b_batch_text.pkl: len=1534\n",
      "output_codeS_beam5_bird_lenpen05_0-5shot_OneSQLQwen-32b_batch_text.pkl: len=1534\n",
      "output_codeS_beam5_bird_lenpen05_0-5shot_OneSQLQwen-32b_batch_text.pkl: len=1534\n",
      "output_codeS_beam5_bird_lenpen05_0-5shot_OneSQLQwen-32b_batch_text.pkl: len=1534\n",
      "output_codeS_beam5_bird_lenpen05_0-5shot_OneSQLQwen-32b_batch_text.pkl: len=1534\n",
      "output_codeS_beam5_bird_lenpen05_0-5shot_OneSQLQwen-32b_batch_text.pkl: len=1534\n",
      "output_codeS_beam5_bird_lenpen05_0-5shot_OneSQLQwen-32b_batch_text.pkl: len=1534\n",
      "./MBRoutput_sequences_total-4_batch_text.pkl: len=1534\n",
      "./vulcan_output/output_CodeS_beam4_bird_wEvidence_1shot_SFTCodeS-7b_2ndOf0-5shot_a1b2Joint6comp_beams_log_prob.pkl: len=6136\n",
      "./vulcan_output/output_CodeS_beam4_bird_wEvidence_1shot_SFTCodeS-7b_3rdOf0-5shot_a1b2Joint6comp_beams_log_prob.pkl: len=6136\n",
      "./vulcan_output/output_CodeS_beam4_divpen05_bird_wEvidence_1shot_SFTCodeS-7b_1stOf0-5shot_a1b2Joint6comp_beams_log_prob.pkl: len=6136\n",
      "./vulcan_output/output_CodeS_beam4_divpen05_bird_wEvidence_1shot_SFTCodeS-7b_2ndOf0-5shot_a1b2Joint6comp_beams_log_prob.pkl: len=6136\n",
      "./vulcan_output/output_CodeS_beam4_divpen1_bird_wEvidence_1shot_SFTCodeS-7b_1stOf0-5shot_a1b2Joint6comp_beams_log_prob.pkl: len=6136\n",
      "./vulcan_output/output_CodeS_beam4_divpen1_bird_wEvidence_1shot_SFTCodeS-7b_2ndOf0-5shot_a1b2Joint6comp_beams_log_prob.pkl: len=6136\n",
      "./vulcan_output/output_CodeS_beam4_divpen2_bird_wEvidence_1shot_SFTCodeS-7b_1stOf0-5shot_a1b2Joint6comp_beams_log_prob.pkl: len=6136\n",
      "./vulcan_output/output_CodeS_beam4_divpen2_bird_wEvidence_1shot_SFTCodeS-7b_2ndOf0-5shot_a1b2Joint6comp_beams_log_prob.pkl: len=6136\n",
      "./MBRoutput_sequences_total-4_a1b2Joint6comp_beams_log_prob.pkl: len=1534\n",
      "./vulcan_output/output_CodeS_beam4_bird_wEvidence_1shot_SFTCodeS-7b_2ndOf0-5shot.pkl: len=6136\n",
      "./vulcan_output/output_CodeS_beam4_bird_wEvidence_1shot_SFTCodeS-7b_3rdOf0-5shot.pkl: len=6136\n",
      "./vulcan_output/output_CodeS_beam4_divpen05_bird_wEvidence_1shot_SFTCodeS-7b_1stOf0-5shot.pkl: len=6136\n",
      "./vulcan_output/output_CodeS_beam4_divpen05_bird_wEvidence_1shot_SFTCodeS-7b_2ndOf0-5shot.pkl: len=6136\n",
      "./vulcan_output/output_CodeS_beam4_divpen1_bird_wEvidence_1shot_SFTCodeS-7b_1stOf0-5shot.pkl: len=6136\n",
      "./vulcan_output/output_CodeS_beam4_divpen1_bird_wEvidence_1shot_SFTCodeS-7b_2ndOf0-5shot.pkl: len=6136\n",
      "./vulcan_output/output_CodeS_beam4_divpen2_bird_wEvidence_1shot_SFTCodeS-7b_1stOf0-5shot.pkl: len=6136\n",
      "./vulcan_output/output_CodeS_beam4_divpen2_bird_wEvidence_1shot_SFTCodeS-7b_2ndOf0-5shot.pkl: len=6136\n",
      "./MBRoutput_sequences_total-4.pkl: len=1534\n"
     ]
    }
   ],
   "source": [
    "#Creating files containing the outputs of the components. Each file contains the one type of output: '_batch_text' , '_input_ids' , '_inputs_log_prob' , '_starting_batch_input_len'\n",
    "#Each file is a list of lists: [ [output0_comp0, output0comp1, ..., output0comp5] , [output1_comp0, output1comp1, ..., output1comp5] ...  ]\n",
    "import pickle as pkl\n",
    "#This is for the time when we have input_ids and _starting_batch_input_len:\n",
    "prefixes = ['./vulcan_output/output_CodeS_beam4_bird_wEvidence_1shot_SFTCodeS-7b_2ndOf0-5shot' , './vulcan_output/output_CodeS_beam4_bird_wEvidence_1shot_SFTCodeS-7b_3rdOf0-5shot', './vulcan_output/output_CodeS_beam4_divpen05_bird_wEvidence_1shot_SFTCodeS-7b_1stOf0-5shot',\n",
    "           './vulcan_output/output_CodeS_beam4_divpen05_bird_wEvidence_1shot_SFTCodeS-7b_2ndOf0-5shot' , './vulcan_output/output_CodeS_beam4_divpen1_bird_wEvidence_1shot_SFTCodeS-7b_1stOf0-5shot', './vulcan_output/output_CodeS_beam4_divpen1_bird_wEvidence_1shot_SFTCodeS-7b_2ndOf0-5shot',\n",
    "           './vulcan_output/output_CodeS_beam4_divpen2_bird_wEvidence_1shot_SFTCodeS-7b_1stOf0-5shot', './vulcan_output/output_CodeS_beam4_divpen2_bird_wEvidence_1shot_SFTCodeS-7b_2ndOf0-5shot']\n",
    "# prefixes = ['./outputs2_0_3/output_sequences_total-4' , './outputs4_3_6/output_sequences_total-4', './outputs2_6_9/output_sequences_total-4',\n",
    "#            './outputs4_9_12/output_sequences_total-4' , './outputs4_12_15/output_sequences_total-4']\n",
    "# post_fixes = ['_batch_text' , '_input_ids' , '_inputs_log_prob' , '_starting_batch_input_len' ]\n",
    "# post_fix_candidate_range = [1,1,1,1]\n",
    "\n",
    "##This is for the time when we only have the strings and the probabilities are already with length penalty\n",
    "# prefixes = ['./output_sequences_bird_lenpen05_0-5shot_Qwen7b' , './output_sequences_bird_lenpen05_5-10shot_Qwen7b', './output_sequences_bird_lenpen05_10-15shot_Qwen7b',\n",
    "#            './output_sequences_bird_lenpen05_15-20shot_Qwen7b' , './output_sequences_bird_lenpen05_20-25shot_Qwen7b']\n",
    "post_fixes = ['_batch_text' , '_a1b2Joint6comp_beams_log_prob' , '']\n",
    "# post_fixes = ['_batch_text' , '_input_ids' , ''] # Its only for components in the same calss with \"output_codeS_beam5_bird_lenpen05_0-5shot_OneSQLQwen-32b\"\n",
    "\n",
    "post_fix_candidate_range = [1,4,4]\n",
    "\n",
    "#What do we need for the ensemble?\n",
    "#for every item, lets have a list of lists.\n",
    "for post_fix, post_fix_range in zip(post_fixes, post_fix_candidate_range):\n",
    "    components_list_of_outputs = [ [] for i in prefixes ]\n",
    "    for index , prefix in enumerate(prefixes):\n",
    "        if post_fix == '_batch_text':\n",
    "            prefix = 'output_codeS_beam5_bird_lenpen05_0-5shot_OneSQLQwen-32b'\n",
    "        filename = prefix  + post_fix + '.pkl'\n",
    "        with open(filename , 'rb') as f:\n",
    "            list_of_outputs = pkl.load(f)\n",
    "            print(f'{filename}: len={len(list_of_outputs)}')\n",
    "        # #This might be the code to address the bug happend on handling _starting_batch_input_len for SPIDER dataset with llama.\n",
    "        # if post_fix== '_starting_batch_input_len':\n",
    "        #     print(len(list_of_outputs))\n",
    "        #     new_list_of_outputs = [ele for ele in list_of_outputs[:200] for i in range( 5 )]\n",
    "        #     new_list_of_outputs.extend([ele for ele in list_of_outputs[200:-2] for i in range( 3 )])\n",
    "        #     new_list_of_outputs.extend([list_of_outputs[-2] for i in range( 5 )])\n",
    "        #     new_list_of_outputs.extend([list_of_outputs[-1] for i in range( 2 )])\n",
    "        #     list_of_outputs = new_list_of_outputs\n",
    "        #     print(len(list_of_outputs))\n",
    "            \n",
    "        components_list_of_outputs[index] = list_of_outputs\n",
    "    postfix_total_list = []\n",
    "    for sample_idx in range(0, len(components_list_of_outputs[0]), post_fix_range ):\n",
    "        if post_fix_range>1:\n",
    "            sample_outputs = [ components_list_of_outputs[comp][sample_idx:sample_idx+post_fix_range] for comp in range(len(components_list_of_outputs)) ]\n",
    "        else:\n",
    "            sample_outputs = [ components_list_of_outputs[comp][sample_idx] for comp in range(len(components_list_of_outputs)) ]\n",
    "        postfix_total_list.append(sample_outputs)\n",
    "    # for a, b, c, d, e in zip (components_list_of_outputs[0], components_list_of_outputs[1], components_list_of_outputs[2], components_list_of_outputs[3], components_list_of_outputs[4]):\n",
    "    #     postfix_total_list.append([a,b,c,d,e])\n",
    "    new_prefix = './MBRoutput_sequences_total-4'\n",
    "    output_filename = new_prefix + post_fix + '.pkl'\n",
    "    print(f'{output_filename}: len={len(postfix_total_list)}')\n",
    "    with open(output_filename , 'wb')as f:\n",
    "        pkl.dump(postfix_total_list , f)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b8d4b7-b2b3-4091-b449-8f7500d86c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yadegari/miniconda3/envs/yadegari_cpu/lib/python3.12/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    }
   ],
   "source": [
    "#grouping the pieces of output_sequences\n",
    "import pickle as pkl\n",
    "prefix = './outputs2_0_3/output_sequences_'\n",
    "post_fixes = ['_batch_text' , '_input_ids' , '_inputs_log_prob' , '_starting_batch_input_len' ]\n",
    "# post_fixes = ['_input_ids' , '_inputs_log_prob']\n",
    "for post_fix in post_fixes:\n",
    "    output_sequences = []\n",
    "    for i in range(0 , 1050 , 50):\n",
    "        if i <1000:\n",
    "            if i == 0: \n",
    "                file_name = prefix + '0_'  + str(i+50).lstrip('0') + '-4' + post_fix + '.pkl'\n",
    "            else:\n",
    "                file_name = prefix + str(i).lstrip('0') + '_' + str(i+50).lstrip('0') + '-4' + post_fix + '.pkl'\n",
    "        else:\n",
    "            file_name = prefix + str(i).lstrip('0') + '_end' + '-4' + post_fix + '.pkl'\n",
    "        # print(file_name)\n",
    "        with open(file_name , 'rb') as f:\n",
    "            part_of_output = pkl.load(f)\n",
    "        output_sequences.extend(part_of_output)\n",
    "    with open(prefix + 'total-4' + post_fix + '.pkl' , 'wb')as h:\n",
    "        pkl.dump(output_sequences , h)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
